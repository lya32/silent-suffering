{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend module://ipykernel.pylab.backend_inline version unknown.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import gc\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from IPython.display import Image\n",
    "tqdm.pandas()\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image(filename = \"Model_Structure.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 24603 entries, 0 to 24602\n",
      "Data columns (total 9 columns):\n",
      "aspect                    24603 non-null object\n",
      "course_of_problem         24603 non-null object\n",
      "group                     24603 non-null object\n",
      "id                        24603 non-null object\n",
      "text                      24603 non-null object\n",
      "trainOrtest               24603 non-null object\n",
      "treatment                 24603 non-null object\n",
      "ground_truth_subaspect    24603 non-null object\n",
      "ground_truth_sentiment    24603 non-null int64\n",
      "dtypes: int64(1), object(8)\n",
      "memory usage: 1.7+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 11 columns):\n",
      "aspect                    5000 non-null object\n",
      "course_of_problem         5000 non-null object\n",
      "group                     5000 non-null object\n",
      "id                        5000 non-null object\n",
      "test                      5000 non-null object\n",
      "text                      5000 non-null object\n",
      "trainOrtest               5000 non-null object\n",
      "treatment                 5000 non-null object\n",
      "batch                     5000 non-null int64\n",
      "ground_truth_subaspect    5000 non-null object\n",
      "ground_truth_sentiment    5000 non-null int64\n",
      "dtypes: int64(2), object(9)\n",
      "memory usage: 429.8+ KB\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Load data\n",
    "\"\"\"\n",
    "df = pd.read_excel('medical_sieve_training_set_lower.xlsx')\n",
    "df_t = pd.read_excel('medical_sieve_test_set_lower.xlsx')\n",
    "df.info()\n",
    "df_t.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def process(x):\n",
    "    ans = set()\n",
    "    for each in x[1:-1].split(','): \n",
    "        words = each.strip().strip('\"').strip(\"'\")\n",
    "        m = re.match('(\\w+)-', words)\n",
    "        if m is not None:\n",
    "            ans.add(m[1])\n",
    "#         else:\n",
    "#             ans.add(words)\n",
    "    return list(ans)\n",
    "df['ground_truth_aspect'] = df['ground_truth_subaspect'].apply(lambda x: process(x))\n",
    "df_t['ground_truth_aspect'] = df_t['ground_truth_subaspect'].apply(lambda x: process(x))\n",
    "df_dev = df_t[:2500]\n",
    "df_test = df_t[2500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ground_truth_aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>17239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[access]</td>\n",
       "      <td>2797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[costs]</td>\n",
       "      <td>1613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[trusts]</td>\n",
       "      <td>1071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[errors]</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[delays]</td>\n",
       "      <td>857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[access, costs]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[trusts, costs]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[access, delays]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[errors, access]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[errors, costs]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[delays, costs]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[access, trusts]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               index  ground_truth_aspect\n",
       "0                 []                17239\n",
       "1           [access]                 2797\n",
       "2            [costs]                 1613\n",
       "3           [trusts]                 1071\n",
       "4           [errors]                 1000\n",
       "5           [delays]                  857\n",
       "6    [access, costs]                   14\n",
       "7    [trusts, costs]                    4\n",
       "8   [access, delays]                    3\n",
       "9   [errors, access]                    2\n",
       "10   [errors, costs]                    1\n",
       "11   [delays, costs]                    1\n",
       "12  [access, trusts]                    1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from random import random\n",
    "# for index, each in enumerate(df.loc[:]['ground_truth_aspect']):\n",
    "#     if each == ['not about'] and random() > 0.5 :\n",
    "#         df = df.drop(index=index)\n",
    "df['ground_truth_aspect'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ground_truth_aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>3955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[access]</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[costs]</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[delays]</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[trusts]</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[errors]</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  ground_truth_aspect\n",
       "0        []                 3955\n",
       "1  [access]                  356\n",
       "2   [costs]                  250\n",
       "3  [delays]                  181\n",
       "4  [trusts]                  151\n",
       "5  [errors]                  107"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t['ground_truth_aspect'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ground_truth_aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[access]</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[costs]</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[delays]</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[trusts]</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[errors]</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  ground_truth_aspect\n",
       "0        []                 1962\n",
       "1  [access]                  189\n",
       "2   [costs]                  129\n",
       "3  [delays]                   86\n",
       "4  [trusts]                   81\n",
       "5  [errors]                   53"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dev['ground_truth_aspect'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>ground_truth_aspect</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[access]</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[costs]</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[delays]</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[trusts]</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[errors]</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  ground_truth_aspect\n",
       "0        []                 1993\n",
       "1  [access]                  167\n",
       "2   [costs]                  121\n",
       "3  [delays]                   95\n",
       "4  [trusts]                   70\n",
       "5  [errors]                   54"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test['ground_truth_aspect'].value_counts().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique types of compliant:\n",
      " ['access' 'costs' 'delays' 'errors' 'trusts']\n",
      "Unique sentiments:  [0 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "\n",
    "mlb_aspect = MultiLabelBinarizer()\n",
    "mlb_aspect.fit(df['ground_truth_aspect'])\n",
    "aspect_vectors_train = mlb_aspect.transform(df['ground_truth_aspect'])\n",
    "aspect_vectors_dev = mlb_aspect.transform(df_dev['ground_truth_aspect'])\n",
    "aspect_vectors_test = mlb_aspect.transform(df_test['ground_truth_aspect'])\n",
    "print(\"Unique types of compliant:\\n\",mlb_aspect.classes_)\n",
    "\n",
    "lb_sentiment = LabelBinarizer()\n",
    "lb_sentiment.fit(list(df['ground_truth_sentiment']) + list(df_t['ground_truth_sentiment']))\n",
    "sentiment_vectors_train = lb_sentiment.transform(df['ground_truth_sentiment'])\n",
    "sentiment_vectors_dev = lb_sentiment.transform(df_dev['ground_truth_sentiment'])\n",
    "sentiment_vectors_test = lb_sentiment.transform(df_test['ground_truth_sentiment'])\n",
    "\n",
    "print(\"Unique sentiments: \",lb_sentiment.classes_)\n",
    "lb_group = LabelBinarizer()\n",
    "lb_group.fit(list(df['group']) + list(df_t['group']))\n",
    "group_vectors_train = lb_group.transform(df['group'])\n",
    "group_vectors_dev = lb_group.transform(df_dev['group'])\n",
    "group_vectors_test = lb_group.transform(df_test['group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    16851\n",
      "1     7506\n",
      "2      246\n",
      "Name: ground_truth_sentiment, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZEAAAETCAYAAAD3WTuEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAc+UlEQVR4nO3de7xddX3m8c9jQhAETCAHxCQQlIgGahFTLjpWFIXghWQcqPCSEjCdWAve2o6CdhovpMKMI4IVkEogWIeAiBIFxZRLmVZuQa4BMeEiORDgYBIuIsHAM3+s3ym7h31Odlay9+ZwnvfrtV97re/6rbV+62zYT9ZtL9kmIiKijld0uwMRETF8JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiakuIxEuSpKWS9u92P7pJ0m6Sbpb0pKRPdrEfZ0r6n91af7y0KfeJRKdJuh/4C9v/0lA7utT+ywYsZzJwH7CZ7XWbtpfdJ+ls4Anbn+ngOo9mAz+HNvbli8Cuto/sdl9icNkTiRiEpNFd7sLOwNIu9yFiSAmReEmSdL+k95ThvSUtkfSEpEckfb00u6a8r5H0lKT9JL1e0pWSfivpMUnfkzS2Ybl7NRwi+r6kCySdWKbtL6lX0uckPQycI2mcpJ9I6pO0ugxPbFje1ZJOlPSL0ocfS9qurPcJSTeWPabBtvOQcuhuTVnWm0r9SuBdwD+W5b6hybxHS7q3bMt9kj7SMO2jku4qfb5c0s4N0yzpLyUtK9O/pcqbgDOB/co615T25zb5G31W0qOSVkqaKel9kn4taZWkzzes6xWSjpd0T/lMLpS0bZk2ufRllqQHyuf1hTJtOvB54MOlL7eu5z+Z6BbbeeXV0RdwP/CeAbWjgX9r1ga4FvjzMrwVsG8ZngwYGN0w367Ae4HNgR6qoPlGmTYG+A3wKWAz4EPAs8CJZfr+wDrg5DL/FsB2wH8DtgS2Br4P/KhhfVcDy4HXA68G7gR+DbwHGA2cB5wzyN/hDcDvSn83Az5bljWmYdl/Mci8rwKeAHYr4zsCu5fhmWU5byp9+DvgFw3zGvgJMBbYCegDpjf7HErt3CZ/o78vff7vZf7/W/4+uwPPAK8r7T8NXAdMLH/TbwPnD/j8/qn8rf8YWAu8qUz/IvDP3f7vNa+hX9kTiW75UfnX95ryL97Th2j7B2BXSeNtP2X7usEa2l5ue7Httbb7gK8D7yyT96X6Uj3N9h9sXwzcMGARzwNzy/y/t/1b2z+w/bTtJ4F5Dcvrd47te2w/DvwUuMf2v7g6T/N94C2DdPfDwKWlv38Avkb1Zfq2If4WA/u6h6QtbK+03X/o62PAV23fVfrwD8CejXsjwEm219h+ALgK2LPFdUL1ecwrfV4IjAdOtf1k6cNS4M0NffmC7V7ba6mC4dABhwq/VP7WtwK3UoVJDBMJkeiWmbbH9r+Avxqi7Wyqf7X/qhwe+sBgDSVtL2mhpAclPQH8M9WXHMBrgQdtN15NsmLAIvpsP9OwvC0lfVvSb8ryrgHGShrVMM8jDcO/bzK+1SDdfS3VnhEAtp8v/Zkw2PY1tP0dVQj9JbBS0qWS3lgm7wyc2hDQqwANWO7DDcNPD9HHZn5r+7ky/PvyPtg27wz8sKEvdwHPATtsor5ElyVE4iXP9jLbRwDbUx1qukjSq6gOhQz01VJ/s+1tgCOpvkABVgITJKmh/aSBqxsw/jfAbsA+ZXl/Wupi4z1E9SVbLbDq1yTgwVZmtn257fdSHcr6FdVhIaiC6GONIW17C9u/aGWxG7QF67cCOHhAX15pu5VtzKWjw0BCJF7yJB0pqaf8S31NKT9HdSz+eeB1Dc23Bp6iOtk+AfgfDdOuLfMdJ2m0pBnA3utZ/dZU/7JeU04Iz93oDXrBhcD7JR0gaTOqwFoLrPfLXtIO5aT8q8o8T1FtG1Qnx0+QtHtp+2pJh7XYp0eAiZLGbOC2DOZMYF7/oTRJPeXv3mpfJkvK99RLWD6cGA6mA0slPQWcChxu+xnbT1Odo/j3crhkX+BLwF7A48ClwMX9C7H9LNXJ9NlUYXQk1QnmtUOs+xtU5ykeozpB/LNNtVG27y59+GZZ/geBD5Z+rs8rqELnIarDVe+kHBK0/UOqPbaF5RDcHcDBLXbrSqpzGg9Leqz1rRnUqcAi4OeSnqT6G+7T4rzfL++/lfTLTdCXaIPcbBgjmqTrgTNtn9PtvkQMR9kTiRFF0jslvaYczppFdRXRJtu7iBhpun1HbkSn7UZ1LmIr4B7gUNsru9uliOErh7MiIqK2HM6KiIjaEiIREVHbiDsnMn78eE+ePLnb3YiIGFZuuummx2z3DKyPuBCZPHkyS5Ys6XY3IiKGFUm/aVbP4ayIiKgtIRIREbUlRCIioraESERE1JYQiYiI2hIiERFRW0IkIiJqS4hERERtI+5mw06bfPyl3e5C29x/0vu73YWI6LLsiURERG0JkYiIqC0hEhERtSVEIiKitoRIRETUlhCJiIja2hYikuZLelTSHQPqn5B0t6Slkv5XQ/0EScvLtIMa6tNLbbmk4xvqu0i6XtIySRdIGtOubYmIiObauSdyLjC9sSDpXcAM4M22dwe+VupTgcOB3cs8p0saJWkU8C3gYGAqcERpC3AycIrtKcBqYHYbtyUiIppoW4jYvgZYNaD8ceAk22tLm0dLfQaw0PZa2/cBy4G9y2u57XttPwssBGZIEvBu4KIy/wJgZru2JSIimuv0OZE3AO8oh6H+VdKflPoEYEVDu95SG6y+HbDG9roB9YiI6KBO/+zJaGAcsC/wJ8CFkl4HqElb0zzkPET7piTNAeYA7LTTThvY5YiIGEyn90R6gYtduQF4Hhhf6pMa2k0EHhqi/hgwVtLoAfWmbJ9le5rtaT09PZtsYyIiRrpOh8iPqM5lIOkNwBiqQFgEHC5pc0m7AFOAG4AbgSnlSqwxVCffF9k2cBVwaFnuLOCSjm5JRES073CWpPOB/YHxknqBucB8YH657PdZYFYJhKWSLgTuBNYBx9p+riznOOByYBQw3/bSsorPAQslnQjcDJzdrm2JiIjm2hYito8YZNKRg7SfB8xrUr8MuKxJ/V6qq7ciIqJLcsd6RETUlhCJiIjaEiIREVFbQiQiImpLiERERG0JkYiIqC0hEhERtSVEIiKitoRIRETUlhCJiIjaEiIREVFbQiQiImpLiERERG0JkYiIqC0hEhERtSVEIiKitraFiKT5kh4tTzEcOO1vJVnS+DIuSadJWi7pNkl7NbSdJWlZec1qqL9V0u1lntMkqV3bEhERzbVzT+RcYPrAoqRJwHuBBxrKB1M9V30KMAc4o7TdluqxuvtQPcVwrqRxZZ4zStv++V60roiIaK+2hYjta4BVTSadAnwWcENtBnCeK9cBYyXtCBwELLa9yvZqYDEwvUzbxva15Rnt5wEz27UtERHRXEfPiUg6BHjQ9q0DJk0AVjSM95baUPXeJvXB1jtH0hJJS/r6+jZiCyIiolHHQkTSlsAXgL9vNrlJzTXqTdk+y/Y029N6enpa6W5ERLSgk3sirwd2AW6VdD8wEfilpNdQ7UlMamg7EXhoPfWJTeoREdFBHQsR27fb3t72ZNuTqYJgL9sPA4uAo8pVWvsCj9teCVwOHChpXDmhfiBweZn2pKR9y1VZRwGXdGpbIiKi0s5LfM8HrgV2k9QrafYQzS8D7gWWA/8E/BWA7VXAV4Aby+vLpQbwceA7ZZ57gJ+2YzsiImJwo9u1YNtHrGf65IZhA8cO0m4+ML9JfQmwx8b1MiIiNkbuWI+IiNoSIhERUVtCJCIiakuIREREbQmRiIioLSESERG1JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiakuIREREbQmRiIioLSESERG1JUQiIqK2dj6Uar6kRyXd0VD735J+Jek2ST+UNLZh2gmSlku6W9JBDfXppbZc0vEN9V0kXS9pmaQLJI1p17ZERERz7dwTOReYPqC2GNjD9puBXwMnAEiaChwO7F7mOV3SKEmjgG8BBwNTgSNKW4CTgVNsTwFWA0M9OTEiItqgbSFi+xpg1YDaz22vK6PXARPL8Axgoe21tu+jeuTt3uW13Pa9tp8FFgIzynPV3w1cVOZfAMxs17ZERERz3Twn8lFeeC76BGBFw7TeUhusvh2wpiGQ+usREdFBXQkRSV8A1gHf6y81aeYa9cHWN0fSEklL+vr6NrS7ERExiI6HiKRZwAeAj9ju/+LvBSY1NJsIPDRE/TFgrKTRA+pN2T7L9jTb03p6ejbNhkRERGdDRNJ04HPAIbafbpi0CDhc0uaSdgGmADcANwJTypVYY6hOvi8q4XMVcGiZfxZwSae2IyIiKu28xPd84FpgN0m9kmYD/whsDSyWdIukMwFsLwUuBO4EfgYca/u5cs7jOOBy4C7gwtIWqjD6a0nLqc6RnN2ubYmIiOZGr79JPbaPaFIe9Ive9jxgXpP6ZcBlTer3Ul29FRERXZI71iMioraESERE1JYQiYiI2hIiERFRW0IkIiJqS4hERERtCZGIiKgtIRIREbUlRCIioraESERE1JYQiYiI2hIiERFRW0IkIiJqS4hERERtCZGIiKgtIRIREbW188mG8yU9KumOhtq2khZLWlbex5W6JJ0mabmk2yTt1TDPrNJ+WXk+e3/9rZJuL/OcJknt2paIiGiunXsi5wLTB9SOB66wPQW4oowDHEz1XPUpwBzgDKhCB5gL7EP1FMO5/cFT2sxpmG/guiIios3aFiK2rwFWDSjPABaU4QXAzIb6ea5cB4yVtCNwELDY9irbq4HFwPQybRvb19o2cF7DsiIiokM6fU5kB9srAcr79qU+AVjR0K631Iaq9zapR0REB71UTqw3O5/hGvXmC5fmSFoiaUlfX1/NLkZExECdDpFHyqEoyvujpd4LTGpoNxF4aD31iU3qTdk+y/Y029N6eno2eiMiIqLSUohIuqKVWgsWAf1XWM0CLmmoH1Wu0toXeLwc7rocOFDSuHJC/UDg8jLtSUn7lquyjmpYVkREdMjooSZKeiWwJTC+fIn3H0baBnjteuY9H9i/zNtLdZXVScCFkmYDDwCHleaXAe8DlgNPA8cA2F4l6SvAjaXdl233n6z/ONUVYFsAPy2viIjooCFDBPgY8GmqwLiJF0LkCeBbQ81o+4hBJh3QpK2BYwdZznxgfpP6EmCPofoQERHtNWSI2D4VOFXSJ2x/s0N9ioiIYWJ9eyIA2P6mpLcBkxvnsX1em/oVERHDQEshIum7wOuBW4DnSrn/Jr+IiBihWgoRYBowtZy7iIiIAFq/T+QO4DXt7EhERAw/re6JjAfulHQDsLa/aPuQtvQqIiKGhVZD5Ivt7ERERAxPrV6d9a/t7khERAw/rV6d9SQv/MDhGGAz4He2t2lXxyIi4qWv1T2RrRvHJc2kekhURESMYLV+xdf2j4B3b+K+RETEMNPq4awPNYy+guq+kdwzEhExwrV6ddYHG4bXAfdTPdI2IiJGsFbPiRzT7o5ERMTw0+pDqSZK+qGkRyU9IukHkiauf86IiHg5a/XE+jlUTx98LTAB+HGp1SLpM5KWSrpD0vmSXilpF0nXS1om6QJJY0rbzcv48jJ9csNyTij1uyUdVLc/ERFRT6sh0mP7HNvryutcoNbDyiVNAD4JTLO9BzAKOBw4GTjF9hRgNTC7zDIbWG17V+CU0g5JU8t8uwPTgdMljarTp4iIqKfVEHlM0pGSRpXXkcBvN2K9o4EtJI2mevzuSqpLhi8q0xcAM8vwjDJOmX5Aea76DGCh7bW276N6tG7uXYmI6KBWQ+SjwJ8BD1N94R9KeQ76hrL9IPA1qmesrwQep3r07hrb60qzXqrDZpT3FWXedaX9do31JvNEREQHtBoiXwFm2e6xvT1VqHyxzgoljaPai9iF6hzLq4CDmzTtvw9Fg0wbrN5snXMkLZG0pK+vb8M7HRERTbUaIm+2vbp/xPYq4C011/ke4D7bfbb/AFwMvA0YWw5vAUwEHirDvcAkgDL91cCqxnqTef4T22fZnmZ7Wk9PrVM5ERHRRKsh8oqyBwGApG1p/UbFgR4A9pW0ZTm3cQBwJ3AV1WEygFnAJWV4URmnTL+yPGFxEXB4uXprF2AKcEPNPkVERA2tBsH/AX4h6SKqQ0Z/Bsyrs0Lb15fl/JLq7vebgbOAS4GFkk4stbPLLGcD35W0nGoP5PCynKWSLqQKoHXAsbafIyIiOqbVO9bPk7SE6goqAR+yfWfdldqeC8wdUL6XJldX2X4GOGyQ5cyjZphFRMTGa/mQVAmN2sEREREvP3XPa0S87E0+/tJud6Gt7j/p/d3uQrwM1HqeSEREBCREIiJiIyREIiKitoRIRETUlhCJiIjaEiIREVFbQiQiImpLiERERG0JkYiIqC0hEhERtSVEIiKitoRIRETUlhCJiIjaEiIREVFbV0JE0lhJF0n6laS7JO0naVtJiyUtK+/jSltJOk3Sckm3SdqrYTmzSvtlkmYNvsaIiGiHbu2JnAr8zPYbgT8G7gKOB66wPQW4oowDHEz1/PQpwBzgDPiP57zPBfaheiLi3MbnwEdERPt1PEQkbQP8KeUZ6raftb0GmAEsKM0WADPL8AzgPFeuA8ZK2hE4CFhse5Xt1cBiYHoHNyUiYsTrxp7I64A+4BxJN0v6jqRXATvYXglQ3rcv7ScAKxrm7y21weoREdEh3QiR0cBewBm23wL8jhcOXTWjJjUPUX/xAqQ5kpZIWtLX17eh/Y2IiEF0I0R6gV7b15fxi6hC5ZFymIry/mhD+0kN808EHhqi/iK2z7I9zfa0np6eTbYhEREjXcdDxPbDwApJu5XSAcCdwCKg/wqrWcAlZXgRcFS5Smtf4PFyuOty4EBJ48oJ9QNLLSIiOmR0l9b7CeB7ksYA9wLHUAXahZJmAw8Ah5W2lwHvA5YDT5e22F4l6SvAjaXdl22v6twmREREV0LE9i3AtCaTDmjS1sCxgyxnPjB/0/YuIiJalTvWIyKitoRIRETUlhCJiIjaEiIREVFbQiQiImpLiERERG0JkYiIqC0hEhERtSVEIiKitoRIRETUlhCJiIjaEiIREVFbQiQiImpLiERERG0JkYiIqK1rISJplKSbJf2kjO8i6XpJyyRdUB5YhaTNy/jyMn1ywzJOKPW7JR3UnS2JiBi5urkn8ingrobxk4FTbE8BVgOzS302sNr2rsAppR2SpgKHA7sD04HTJY3qUN8jIoIuhYikicD7ge+UcQHvBi4qTRYAM8vwjDJOmX5AaT8DWGh7re37qB6fu3dntiAiIqB7eyLfAD4LPF/GtwPW2F5XxnuBCWV4ArACoEx/vLT/j3qTeSIiogM6HiKSPgA8avumxnKTpl7PtKHmGbjOOZKWSFrS19e3Qf2NiIjBdWNP5O3AIZLuBxZSHcb6BjBW0ujSZiLwUBnuBSYBlOmvBlY11pvM85/YPsv2NNvTenp6Nu3WRESMYB0PEdsn2J5oezLVifErbX8EuAo4tDSbBVxShheVccr0K2271A8vV2/tAkwBbujQZkREBDB6/U065nPAQkknAjcDZ5f62cB3JS2n2gM5HMD2UkkXAncC64BjbT/X+W5HRIxcXQ0R21cDV5fhe2lydZXtZ4DDBpl/HjCvfT2MiIih5I71iIioLSESERG1JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiakuIREREbQmRiIioLSESERG1JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiakuIREREbR0PEUmTJF0l6S5JSyV9qtS3lbRY0rLyPq7UJek0Scsl3SZpr4ZlzSrtl0maNdg6IyKiPbqxJ7IO+BvbbwL2BY6VNBU4HrjC9hTgijIOcDDV89OnAHOAM6AKHWAusA/VExHn9gdPRER0RsdDxPZK278sw08CdwETgBnAgtJsATCzDM8AznPlOmCspB2Bg4DFtlfZXg0sBqZ3cFMiIka8rp4TkTQZeAtwPbCD7ZVQBQ2wfWk2AVjRMFtvqQ1Wj4iIDulaiEjaCvgB8GnbTwzVtEnNQ9SbrWuOpCWSlvT19W14ZyMioqmuhIikzagC5Hu2Ly7lR8phKsr7o6XeC0xqmH0i8NAQ9RexfZbtaban9fT0bLoNiYgY4bpxdZaAs4G7bH+9YdIioP8Kq1nAJQ31o8pVWvsCj5fDXZcDB0oaV06oH1hqERHRIaO7sM63A38O3C7pllL7PHAScKGk2cADwGFl2mXA+4DlwNPAMQC2V0n6CnBjafdl26s6swkREQFdCBHb/0bz8xkABzRpb+DYQZY1H5i/6XoXEREbInesR0REbQmRiIioLSESERG1JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiakuIREREbQmRiIioLSESERG1JUQiIqK2hEhERNSWEImIiNoSIhERUVtCJCIiauvGkw03KUnTgVOBUcB3bJ/U5S5FxEvA5OMv7XYX2ur+k97f7S4Aw3xPRNIo4FvAwcBU4AhJU7vbq4iIkWNYhwiwN7Dc9r22nwUWAjO63KeIiBFjuB/OmgCsaBjvBfYZ2EjSHGBOGX1K0t0d6Fu3jAce68SKdHIn1jKidOyzg3x+bfBy//x2blYc7iGiJjW/qGCfBZzV/u50n6Qltqd1ux+x4fLZDW8j9fMb7oezeoFJDeMTgYe61JeIiBFnuIfIjcAUSbtIGgMcDizqcp8iIkaMYX04y/Y6SccBl1Nd4jvf9tIud6vbRsRhu5epfHbD24j8/GS/6BRCRERES4b74ayIiOiihEhERNSWEImIiNqG9Yn1kU7SG6nu0J9AdX/MQ8Ai23d1tWMRL3Pl/70JwPW2n2qoT7f9s+71rPOyJzJMSfoc1c+8CLiB6nJnAedLOr6bfYuNI+mYbvchBifpk8AlwCeAOyQ1/tTSP3SnV92Tq7OGKUm/Bna3/YcB9THAUttTutOz2FiSHrC9U7f7Ec1Juh3Yz/ZTkiYDFwHftX2qpJttv6WrHeywHM4avp4HXgv8ZkB9xzItXsIk3TbYJGCHTvYlNtio/kNYtu+XtD9wkaSdaf5TTC9rCZHh69PAFZKW8cKPUO4E7Aoc17VeRat2AA4CVg+oC/hF57sTG+BhSXvavgWg7JF8AJgP/FF3u9Z5CZFhyvbPJL2B6ufwJ1B9+fQCN9p+rqudi1b8BNiq/4uokaSrO9+d2ABHAesaC7bXAUdJ+nZ3utQ9OScSERG15eqsiIioLSESERG1JUQiWiTpC5KWSrpN0i2SXvQUzRaWsaek9zWMH9Lu+3ok7S/pbe1cR4xcObEe0QJJ+wEfAPayvVbSeGBMjUXtCUwDLgOwvYj2PwNnf+ApctVXtEFOrEe0QNKHgGNsf3BA/a3A14GtqJ6vfbTtleUKq+uBdwFjgdllfDmwBfAg8NUyPM32cZLOBX4PvJHqedbHALOA/ah+XuPoss4DgS8BmwP3lH49Jel+YAHwQWAz4DDgGeA64DmgD/iE7f+3af86MZLlcFZEa34OTJL0a0mnS3qnpM2AbwKH2n4r1X0C8xrmGW17b6p7eubafhb4e+AC23vavqDJesYB7wY+A/wYOAXYHfijcihsPPB3wHts7wUsAf66Yf7HSv0M4G9t3w+cCZxS1pkAiU0qh7MiWlD+pf9W4B1UexcXACcCewCLJUH1dM2VDbNdXN5vAia3uKof23b5aY1HbN8OIGlpWcZEYCrw72WdY4BrB1nnh1rfwoh6EiIRLSo3cV4NXF2+5I+l+p2y/QaZZW15f47W/1/rn+f5huH+8dFlWYttH7EJ1xlRWw5nRbRA0m6SGn/Uck/gLqCnnHRH0maSdl/Pop4Ett6IrlwHvF3SrmWdW5ZfLmjnOiMGlRCJaM1WwAJJd5YfT5xKdX7jUOBkSbcCtwDru5T2KmBquUT4wxvaCdt9wNFUP/l/G1WovHE9s/0Y+K9lne/Y0HVGDCVXZ0VERG3ZE4mIiNoSIhERUVtCJCIiakuIREREbQmRiIioLSESERG1JUQiIqK2hEhERNT2/wFbbFtF6WR2JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df['ground_truth_sentiment'].value_counts())\n",
    "df['ground_truth_sentiment'].value_counts().plot.bar()\n",
    "plt.title('Histagram of sentiment')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'count')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'].apply(lambda x: len(x.split())).hist(grid=False)\n",
    "plt.title('Histagram of length of per sentense')\n",
    "plt.xlabel('the length of per sentense')\n",
    "plt.ylabel('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aspect_out_put: 5, senntiment_out_put: 3, group_dim: 277\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LEN = 128\n",
    "MAX_FEATURES = 481731\n",
    "ASPECT_OUTPUT_DIM = len(mlb_aspect.classes_)\n",
    "SENTIMENT_OUT_DIM = len(lb_sentiment.classes_)\n",
    "GROUP_DIM = len(lb_group.classes_)\n",
    "print(\"aspect_out_put: {}, senntiment_out_put: {}, group_dim: {}\".format(ASPECT_OUTPUT_DIM, SENTIMENT_OUT_DIM, GROUP_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Fitting Tokenizer\n",
      "INFO:root:Beginning process train text\n",
      "INFO:root:Beginning process dev text\n",
      "INFO:root:Beginning process test text\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fit tokenizer\n",
    "\"\"\"\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "logging.info(\"Fitting Tokenizer\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_FEATURES, lower=True)\n",
    "# Updates internal vocabulary based on a list of texts\n",
    "whole_text = []\n",
    "with open('medical_sieve_sentence_corpus.txt', 'r') as f:\n",
    "    for each in f.readlines():\n",
    "        whole_text.append(each.strip())\n",
    "f.close()\n",
    "tokenizer.fit_on_texts(whole_text)\n",
    "\n",
    "logging.info(\"Beginning process train text\")\n",
    "train_text = tokenizer.texts_to_sequences(df['text'])\n",
    "train_text = pad_sequences(train_text, maxlen=MAX_SEQUENCE_LEN)\n",
    "\n",
    "logging.info(\"Beginning process dev text\")\n",
    "dev_text = tokenizer.texts_to_sequences(df_dev['text'])\n",
    "dev_text = pad_sequences(dev_text, maxlen=MAX_SEQUENCE_LEN)\n",
    "\n",
    "logging.info(\"Beginning process test text\")\n",
    "test_text = tokenizer.texts_to_sequences(df_test['text'])\n",
    "test_text = pad_sequences(test_text, maxlen=MAX_SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 389618\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5095"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print('length:', len(word_index))\n",
    "# del tokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get word Embedding\n",
    "\"\"\"\n",
    "EMB_PATH = \"cbow_model_medical_sieve_lower.vec\"\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def get_coefs(word,*arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def load_embeddings(embed_dir=EMB_PATH):\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in tqdm(open(embed_dir)))\n",
    "    return embedding_index\n",
    "\n",
    "# EMB_PATH  = \"glove.840B.300d.txt\"\n",
    "# EMBEDDING_DIM = 300\n",
    "\n",
    "# def load_embeddings(embedding_path=EMB_PATH):\n",
    "#     '''return a dict whose key is word, value is pretrained word embedding'''\n",
    "#     embeddings_index = {}\n",
    "#     f = open(embedding_path, 'r', encoding='utf-8')\n",
    "#     for line in f:\n",
    "#         values = line.split()\n",
    "#         try:\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#         except:\n",
    "#             print(\"Err on \", values[:2])\n",
    "#     f.close()\n",
    "#     print('Total %s word vectors.' % len(embeddings_index))\n",
    "#     return embeddings_index\n",
    "\n",
    "def build_matrix(word_index, embeddings_index):\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1,EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embeddings_index[word]\n",
    "        except:\n",
    "            embedding_matrix[i] = embeddings_index[\"unknown\"]\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92302it [00:05, 17249.82it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index = load_embeddings()\n",
    "embedding_matrix = build_matrix(word_index, embeddings_index)\n",
    "del embeddings_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Attention model\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        \n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer = self.init,\n",
    "                                 name = '{}_W'.format(self.name),\n",
    "                                 regularizer = self.W_regularizer,\n",
    "                                 constraint = self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "        \n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer = 'zero',\n",
    "                                     name = '{}_b'.format(self.name),\n",
    "                                     regularizer = self.b_regularizer,\n",
    "                                     constraint = self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        \n",
    "        self.built = True\n",
    "    \n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "    \n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        \n",
    "        e_ij = K.reshape(K.dot(\n",
    "                            K.reshape(x, (-1, features_dim)),\n",
    "                            K.reshape(self.W, (features_dim, 1))\n",
    "                    ), (-1, step_dim))\n",
    "        if self.bias:\n",
    "            e_ij += self.b\n",
    "        \n",
    "        e_ij = K.tanh(e_ij)\n",
    "        \n",
    "        a = K.exp(e_ij)\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "        \n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(),\n",
    "                    K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Model builder\n",
    "\"\"\"\n",
    "\n",
    "from keras.layers import SpatialDropout1D, Bidirectional, Dense, LSTM\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Input, Embedding, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adagrad, Adam\n",
    "from bpmll import bp_mll_loss\n",
    "from keras import backend as K\n",
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def build_model(verbose = False, compile = True):\n",
    "    sequence_input = Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32')\n",
    "    embedding_layer = Embedding(len(word_index) + 1,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LEN,\n",
    "                                trainable=False)\n",
    "    \n",
    "    x = embedding_layer(sequence_input)\n",
    "    x = SpatialDropout1D(0.25)(x)        \n",
    "    x = Bidirectional(LSTM(64, return_sequences=True))(x)\n",
    "    \n",
    "    attention = Attention(MAX_SEQUENCE_LEN)(x)\n",
    "    conv = Conv1D(64, kernel_size = 3, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n",
    "    avg_pool1 = GlobalAveragePooling1D()(conv)\n",
    "    max_pool1 = GlobalMaxPooling1D()(conv)\n",
    "    \n",
    "    avg_pool2 = GlobalAveragePooling1D()(x)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x)\n",
    "\n",
    "#     group_input = Input(shape=(GROUP_DIM,), dtype='float32')\n",
    "    x = concatenate([attention, \n",
    "                     avg_pool1, max_pool1, \n",
    "                     avg_pool2, max_pool2])\n",
    "    \n",
    "    preds = Dense(ASPECT_OUTPUT_DIM, activation='sigmoid')(x)\n",
    "    model = Model(sequence_input, preds)\n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    # the loss for sentiment analysis\n",
    "#     if compile:\n",
    "#         model.compile(loss='binary_crossentropy',\n",
    "#                       optimizer=Adam(0.005),\n",
    "#                       metrics=['acc'])\n",
    "    # the loss for aspect classification\n",
    "    if compile:\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='Adam',\n",
    "                      metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 128)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 128, 300)     116885700   input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 128, 300)     0           embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 128, 128)     186880      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 126, 64)      24640       bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "attention_2 (Attention)         (None, 128)          256         bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_3 (Glo (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_4 (Glo (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 128)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           attention_2[0][0]                \n",
      "                                                                 global_average_pooling1d_3[0][0] \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_average_pooling1d_4[0][0] \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5)            2565        concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 117,100,041\n",
      "Trainable params: 214,341\n",
      "Non-trainable params: 116,885,700\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24603 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "24603/24603 [==============================] - 71s 3ms/step - loss: 0.2621 - acc: 0.9235 - val_loss: 0.1740 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.17401, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 2/100\n",
      "24603/24603 [==============================] - 65s 3ms/step - loss: 0.2027 - acc: 0.9399 - val_loss: 0.1435 - val_acc: 0.9578\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.17401 to 0.14352, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 3/100\n",
      "24603/24603 [==============================] - 66s 3ms/step - loss: 0.1677 - acc: 0.9424 - val_loss: 0.1248 - val_acc: 0.9563\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.14352 to 0.12485, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 4/100\n",
      "24603/24603 [==============================] - 66s 3ms/step - loss: 0.1432 - acc: 0.9481 - val_loss: 0.1078 - val_acc: 0.9610\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.12485 to 0.10779, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 5/100\n",
      "24603/24603 [==============================] - 66s 3ms/step - loss: 0.1248 - acc: 0.9529 - val_loss: 0.1097 - val_acc: 0.9574\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.10779\n",
      "Epoch 6/100\n",
      "24603/24603 [==============================] - 66s 3ms/step - loss: 0.1124 - acc: 0.9570 - val_loss: 0.1026 - val_acc: 0.9586\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.10779 to 0.10264, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 7/100\n",
      "24603/24603 [==============================] - 67s 3ms/step - loss: 0.1054 - acc: 0.9587 - val_loss: 0.0978 - val_acc: 0.9608\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.10264 to 0.09783, saving model to Pooled_RNN_ATTENTION_TEXT_CNN_weights_512.hdf5\n",
      "Epoch 8/100\n",
      "24603/24603 [==============================] - 66s 3ms/step - loss: 0.0991 - acc: 0.9608 - val_loss: 0.1054 - val_acc: 0.9547\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.09783\n",
      "Epoch 9/100\n",
      "24603/24603 [==============================] - 67s 3ms/step - loss: 0.0939 - acc: 0.9622 - val_loss: 0.1026 - val_acc: 0.9554\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.09783\n",
      "Epoch 10/100\n",
      "24603/24603 [==============================] - 67s 3ms/step - loss: 0.0905 - acc: 0.9638 - val_loss: 0.1063 - val_acc: 0.9541\n",
      "\n",
      "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.09783\n",
      "Epoch 11/100\n",
      "24603/24603 [==============================] - 67s 3ms/step - loss: 0.0857 - acc: 0.9655 - val_loss: 0.1034 - val_acc: 0.9558\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.09783\n",
      "Epoch 12/100\n",
      "24603/24603 [==============================] - 67s 3ms/step - loss: 0.0847 - acc: 0.9662 - val_loss: 0.0989 - val_acc: 0.9582\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.09783\n",
      "Epoch 00012: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training Session\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', mlb_aspect.classes_, df['ground_truth_aspect'].apply(lambda x: x[0]))\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, mode='min', verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"Pooled_RNN_ATTENTION_TEXT_CNN_weights_\"+str(BATCH_SIZE)+\".hdf5\", verbose=1, save_best_only=True)\n",
    "model = build_model()\n",
    "model.fit(x = train_text,\n",
    "          y = aspect_vectors_train,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = NUM_EPOCHS,\n",
    "          validation_data = (dev_text,\n",
    "                             aspect_vectors_dev),\n",
    "          callbacks = [reduce_lr, es, checkpointer]\n",
    "#           class_weight=class_weights\n",
    "         )\n",
    "model.load_weights(\"Pooled_RNN_ATTENTION_TEXT_CNN_weights_\"+str(BATCH_SIZE)+\".hdf5\")\n",
    "# test_preds /= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def combinations(nums):\n",
    "    ans = [[]]\n",
    "    for row in nums:\n",
    "        curr = []\n",
    "        for combination in ans:\n",
    "            for element in row:\n",
    "                new_combination = copy.deepcopy(combination)\n",
    "                new_combination.append(element)\n",
    "                curr.append(new_combination)\n",
    "        ans = curr\n",
    "    return ans\n",
    "thresholds = [[0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55] for each in range(5)]\n",
    "thresholds_set = combinations(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(dev_text)\n",
    "aspect_vectors = aspect_vectors_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold set: [0.5, 0.3, 0.4, 0.4, 0.2]\n",
      "Confusion Matrix for Each Aspect:\n",
      "============================================================\n",
      "[[[2169  142]\n",
      "  [  41  148]]\n",
      "\n",
      " [[2282   89]\n",
      "  [  23  106]]\n",
      "\n",
      " [[2363   51]\n",
      "  [  37   49]]\n",
      "\n",
      " [[2410   37]\n",
      "  [  29   24]]\n",
      "\n",
      " [[2324   95]\n",
      "  [  43   38]]]\n",
      "Result of Metrics for Evaluation:\n",
      "============================================================\n",
      "Hamming score: 0.95304\n",
      "Exact accuracy: 0.774\n",
      "Fuzzy accuracy: 0.7764\n",
      "Exact accuracy (exclude negative): 0.6672862453531598\n",
      "Fuzzy accuracy (exclude negative): 0.6784386617100372\n",
      "Average F1 Score:  0.5150699195120949\n",
      "ROC AUC Score:  0.9536387989034881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def f1(matrix):\n",
    "    precision = matrix[1][1]*1.0 / (matrix[0][1] + matrix[1][1])\n",
    "    recall = matrix[1][1]*1.0 / (matrix[1][0] + matrix[1][1])\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "max_avg_f1 = 0\n",
    "max_hamming_score = 0\n",
    "max_exact_accuracy = 0\n",
    "max_fuzzy_accuracy = 0\n",
    "max_fuzzy_accuracy_pos = 0\n",
    "max_exact_accuracy_pos = 0\n",
    "max_avg_rocauc = 0\n",
    "max_confusion_matrix = None\n",
    "max_threshold_set = []\n",
    "\n",
    "for threshold_set in thresholds_set:\n",
    "    predict_softmax = np.zeros(aspect_vectors.shape, dtype=int)\n",
    "    for row_index, row in enumerate(val_preds):\n",
    "        for index, each in enumerate(row):\n",
    "            if each >= threshold_set[index]:\n",
    "                predict_softmax[row_index][index] = 1\n",
    "\n",
    "    hamming_score = 1 - hamming_loss(predict_softmax, aspect_vectors) \n",
    "    num_fuzzy_match = 0\n",
    "    num_fuzzy_match_pos = 0\n",
    "    num_exact_match_pos = 0\n",
    "    num_pos = 0\n",
    "    for true, pre in zip(mlb_aspect.inverse_transform(aspect_vectors), mlb_aspect.inverse_transform(predict_softmax)):\n",
    "        if len(true) != 0: \n",
    "            num_pos += 1\n",
    "        intersect = set(pre).intersection(set(true))\n",
    "        if (len(true)>0 and len(pre)>0 and len(intersect) > 0) or (len(true) == 0 and len(pre) == 0):\n",
    "            num_fuzzy_match += 1\n",
    "        if len(true)>0 and len(pre)>0 and len(intersect) > 0:\n",
    "            num_fuzzy_match_pos += 1\n",
    "        if len(true)>0 and len(pre)>0 and pre == true: \n",
    "            num_exact_match_pos += 1\n",
    "    fuzzy_accuracy = num_fuzzy_match*1.0/len(predict_softmax)\n",
    "    exact_accuracy = accuracy_score(predict_softmax, aspect_vectors)\n",
    "    fuzzy_accuracy_pos =  num_fuzzy_match_pos*1.0/num_pos\n",
    "    exact_accuracy_pos = num_exact_match_pos*1.0/num_pos\n",
    "\n",
    "\n",
    "    class_f1 = []\n",
    "    for aspect, confusion_matrix in zip(mlb_aspect.classes_, multilabel_confusion_matrix(aspect_vectors, predict_softmax)):\n",
    "#         print(aspect, ':',f1(confusion_matrix),'\\n', confusion_matrix, '\\n')\n",
    "        class_f1.append(f1(confusion_matrix))\n",
    "        \n",
    "    rocauc_score = roc_auc_score(aspect_vectors, val_preds, 'weighted')\n",
    "    if np.mean(class_f1) > max_avg_f1:\n",
    "        max_threshold_set = threshold_set\n",
    "        max_avg_f1 = max(max_avg_f1, np.mean(class_f1))\n",
    "        max_hamming_score = hamming_score\n",
    "        max_exact_accuracy = exact_accuracy\n",
    "        max_fuzzy_accuracy = fuzzy_accuracy \n",
    "        max_exact_accuracy_pos = exact_accuracy_pos\n",
    "        max_fuzzy_accuracy_pos = fuzzy_accuracy_pos\n",
    "        max_avg_rocauc = rocauc_score\n",
    "        max_confusion_matrix = multilabel_confusion_matrix(aspect_vectors, predict_softmax)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"threshold set:\", max_threshold_set)\n",
    "print(\"Confusion Matrix for Each Aspect:\\n\" + \"=\"*60)\n",
    "print(max_confusion_matrix)\n",
    "print(\"Result of Metrics for Evaluation:\\n\" + \"=\"*60)\n",
    "print(\"Hamming score:\", max_hamming_score)\n",
    "print(\"Exact accuracy:\", max_exact_accuracy)\n",
    "print(\"Fuzzy accuracy:\", max_fuzzy_accuracy)\n",
    "print(\"Exact accuracy (exclude negative):\", max_exact_accuracy_pos )\n",
    "print(\"Fuzzy accuracy (exclude negative):\", max_fuzzy_accuracy_pos)\n",
    "print(\"Average F1 Score: \", max_avg_f1)\n",
    "print(\"ROC AUC Score: \", max_avg_rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_dev_predict.txt', train_val_preds, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(test_text)\n",
    "aspect_vectors = aspect_vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold set: [0.55, 0.2, 0.25, 0.4, 0.25]\n",
      "Confusion Matrix for Each Aspect:\n",
      "============================================================\n",
      "[[[2169  164]\n",
      "  [  32  135]]\n",
      "\n",
      " [[2287   92]\n",
      "  [  21  100]]\n",
      "\n",
      " [[2324   81]\n",
      "  [  36   59]]\n",
      "\n",
      " [[2403   43]\n",
      "  [  23   31]]\n",
      "\n",
      " [[2369   61]\n",
      "  [  37   33]]]\n",
      "Result of Metrics for Evaluation:\n",
      "============================================================\n",
      "Hamming score: 0.9528\n",
      "Exact accuracy: 0.7724\n",
      "Fuzzy accuracy: 0.7748\n",
      "Exact accuracy (exclude negative): 0.6942800788954635\n",
      "Fuzzy accuracy (exclude negative): 0.7061143984220908\n",
      "Average F1 Score:  0.5214636922756722\n",
      "Average ROC AUC Score:  0.9498462538522947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def f1(matrix):\n",
    "    precision = matrix[1][1]*1.0 / (matrix[0][1] + matrix[1][1])\n",
    "    recall = matrix[1][1]*1.0 / (matrix[1][0] + matrix[1][1])\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "max_avg_f1 = 0\n",
    "max_hamming_score = 0\n",
    "max_exact_accuracy = 0\n",
    "max_fuzzy_accuracy = 0\n",
    "max_fuzzy_accuracy_pos = 0\n",
    "max_exact_accuracy_pos = 0\n",
    "max_avg_rocauc = 0\n",
    "max_confusion_matrix = None\n",
    "max_threshold_set = []\n",
    "\n",
    "for threshold_set in thresholds_set:\n",
    "    predict_softmax = np.zeros(aspect_vectors.shape, dtype=int)\n",
    "    for row_index, row in enumerate(val_preds):\n",
    "        for index, each in enumerate(row):\n",
    "            if each >= threshold_set[index]:\n",
    "                predict_softmax[row_index][index] = 1\n",
    "\n",
    "    hamming_score = 1 - hamming_loss(predict_softmax, aspect_vectors) \n",
    "    num_fuzzy_match = 0\n",
    "    num_fuzzy_match_pos = 0\n",
    "    num_exact_match_pos = 0\n",
    "    num_pos = 0\n",
    "    for  true, pre in zip(mlb_aspect.inverse_transform(aspect_vectors), mlb_aspect.inverse_transform(predict_softmax)):\n",
    "        if len(true) != 0: \n",
    "            num_pos += 1\n",
    "        intersect = set(pre).intersection(set(true))\n",
    "        if (len(true)>0 and len(pre)>0 and len(intersect) > 0) or (len(true) == 0 and len(pre) == 0):\n",
    "            num_fuzzy_match += 1\n",
    "        if len(true)>0 and len(pre)>0 and len(intersect) > 0:\n",
    "            num_fuzzy_match_pos += 1\n",
    "        if len(true)>0 and len(pre)>0 and pre == true: \n",
    "            num_exact_match_pos += 1\n",
    "    fuzzy_accuracy = num_fuzzy_match*1.0/len(predict_softmax)\n",
    "    exact_accuracy = accuracy_score(predict_softmax, aspect_vectors)\n",
    "    fuzzy_accuracy_pos =  num_fuzzy_match_pos*1.0/num_pos\n",
    "    exact_accuracy_pos = num_exact_match_pos*1.0/num_pos\n",
    "\n",
    "    rocauc_score = roc_auc_score(aspect_vectors, val_preds, 'weighted')\n",
    "    \n",
    "    class_f1 = []\n",
    "    for aspect, confusion_matrix in zip(mlb_aspect.classes_, multilabel_confusion_matrix(aspect_vectors, predict_softmax)):\n",
    "#         print(aspect, ':',f1(confusion_matrix),'\\n', confusion_matrix, '\\n')\n",
    "        class_f1.append(f1(confusion_matrix))\n",
    "        \n",
    "\n",
    "    if np.mean(class_f1) > max_avg_f1:\n",
    "        max_threshold_set = threshold_set\n",
    "        max_avg_f1 = max(max_avg_f1, np.mean(class_f1))\n",
    "        max_hamming_score = hamming_score\n",
    "        max_exact_accuracy = exact_accuracy\n",
    "        max_fuzzy_accuracy = fuzzy_accuracy \n",
    "        max_exact_accuracy_pos = exact_accuracy_pos\n",
    "        max_fuzzy_accuracy_pos = fuzzy_accuracy_pos\n",
    "        max_avg_rocauc = rocauc_score\n",
    "        max_confusion_matrix = multilabel_confusion_matrix(aspect_vectors, predict_softmax)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"threshold set:\", max_threshold_set)\n",
    "print(\"Confusion Matrix for Each Aspect:\\n\" + \"=\"*60)\n",
    "print(max_confusion_matrix)\n",
    "print(\"Result of Metrics for Evaluation:\\n\" + \"=\"*60)\n",
    "print(\"Hamming score:\", max_hamming_score)\n",
    "print(\"Exact accuracy:\", max_exact_accuracy)\n",
    "print(\"Fuzzy accuracy:\", max_fuzzy_accuracy)\n",
    "print(\"Exact accuracy (exclude negative):\", max_exact_accuracy_pos )\n",
    "print(\"Fuzzy accuracy (exclude negative):\", max_fuzzy_accuracy_pos)\n",
    "print(\"Average F1 Score: \", max_avg_f1)\n",
    "print(\"Average ROC AUC Score: \", max_avg_rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_test_predict.txt', val_preds, delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19682 samples, validate on 4921 samples\n",
      "Epoch 1/100\n",
      "19682/19682 [==============================] - 61s 3ms/step - loss: 0.2741 - acc: 0.9159 - val_loss: 0.2150 - val_acc: 0.9411\n",
      "Epoch 2/100\n",
      "19682/19682 [==============================] - 59s 3ms/step - loss: 0.2095 - acc: 0.9397 - val_loss: 0.1921 - val_acc: 0.9407\n",
      "Epoch 3/100\n",
      "19682/19682 [==============================] - 59s 3ms/step - loss: 0.1859 - acc: 0.9400 - val_loss: 0.1642 - val_acc: 0.9447\n",
      "Epoch 4/100\n",
      "19682/19682 [==============================] - 60s 3ms/step - loss: 0.1552 - acc: 0.9451 - val_loss: 0.1403 - val_acc: 0.9497\n",
      "Epoch 5/100\n",
      "19682/19682 [==============================] - 58s 3ms/step - loss: 0.1342 - acc: 0.9506 - val_loss: 0.1237 - val_acc: 0.9547\n",
      "Epoch 6/100\n",
      "19682/19682 [==============================] - 58s 3ms/step - loss: 0.1203 - acc: 0.9547 - val_loss: 0.1167 - val_acc: 0.9554\n",
      "Epoch 7/100\n",
      "19682/19682 [==============================] - 60s 3ms/step - loss: 0.1115 - acc: 0.9575 - val_loss: 0.1073 - val_acc: 0.9592\n",
      "Epoch 8/100\n",
      "19682/19682 [==============================] - 61s 3ms/step - loss: 0.1045 - acc: 0.9594 - val_loss: 0.1058 - val_acc: 0.9585\n",
      "Epoch 9/100\n",
      "19682/19682 [==============================] - 66s 3ms/step - loss: 0.0996 - acc: 0.9609 - val_loss: 0.1008 - val_acc: 0.9612\n",
      "Epoch 10/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0958 - acc: 0.9619 - val_loss: 0.0995 - val_acc: 0.9617\n",
      "Epoch 11/100\n",
      "19682/19682 [==============================] - 65s 3ms/step - loss: 0.0918 - acc: 0.9632 - val_loss: 0.0955 - val_acc: 0.9627\n",
      "Epoch 12/100\n",
      "19682/19682 [==============================] - 60s 3ms/step - loss: 0.0886 - acc: 0.9647 - val_loss: 0.0965 - val_acc: 0.9616\n",
      "Epoch 13/100\n",
      "19682/19682 [==============================] - 67s 3ms/step - loss: 0.0856 - acc: 0.9653 - val_loss: 0.0930 - val_acc: 0.9629\n",
      "Epoch 14/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.0820 - acc: 0.9671 - val_loss: 0.0961 - val_acc: 0.9619\n",
      "Epoch 15/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.0798 - acc: 0.9670 - val_loss: 0.0950 - val_acc: 0.9624\n",
      "Epoch 16/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.0776 - acc: 0.9689 - val_loss: 0.0946 - val_acc: 0.9623\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 17/100\n",
      "19682/19682 [==============================] - 60s 3ms/step - loss: 0.0745 - acc: 0.9695 - val_loss: 0.0906 - val_acc: 0.9634\n",
      "Epoch 18/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0717 - acc: 0.9711 - val_loss: 0.0937 - val_acc: 0.9626\n",
      "Epoch 19/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0717 - acc: 0.9706 - val_loss: 0.0913 - val_acc: 0.9629\n",
      "Epoch 20/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.0709 - acc: 0.9712 - val_loss: 0.0909 - val_acc: 0.9632\n",
      "\n",
      "Epoch 00020: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 21/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0691 - acc: 0.9723 - val_loss: 0.0911 - val_acc: 0.9633\n",
      "Epoch 22/100\n",
      "19682/19682 [==============================] - 72s 4ms/step - loss: 0.0687 - acc: 0.9723 - val_loss: 0.0924 - val_acc: 0.9626\n",
      "Epoch 00022: early stopping\n",
      "Train on 19682 samples, validate on 4921 samples\n",
      "Epoch 1/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.2554 - acc: 0.9341 - val_loss: 0.2124 - val_acc: 0.9403\n",
      "Epoch 2/100\n",
      "19682/19682 [==============================] - 61s 3ms/step - loss: 0.2068 - acc: 0.9398 - val_loss: 0.1882 - val_acc: 0.9401\n",
      "Epoch 3/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.1802 - acc: 0.9408 - val_loss: 0.1644 - val_acc: 0.9425\n",
      "Epoch 4/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.1549 - acc: 0.9451 - val_loss: 0.1413 - val_acc: 0.9482\n",
      "Epoch 5/100\n",
      "19682/19682 [==============================] - 66s 3ms/step - loss: 0.1367 - acc: 0.9493 - val_loss: 0.1302 - val_acc: 0.9513\n",
      "Epoch 6/100\n",
      "19682/19682 [==============================] - 61s 3ms/step - loss: 0.1229 - acc: 0.9534 - val_loss: 0.1165 - val_acc: 0.9540\n",
      "Epoch 7/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.1135 - acc: 0.9567 - val_loss: 0.1106 - val_acc: 0.9563\n",
      "Epoch 8/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.1053 - acc: 0.9590 - val_loss: 0.1081 - val_acc: 0.9573\n",
      "Epoch 9/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.0997 - acc: 0.9608 - val_loss: 0.1040 - val_acc: 0.9583\n",
      "Epoch 10/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0953 - acc: 0.9630 - val_loss: 0.1018 - val_acc: 0.9596\n",
      "Epoch 11/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.0904 - acc: 0.9642 - val_loss: 0.1013 - val_acc: 0.9593\n",
      "Epoch 12/100\n",
      "19682/19682 [==============================] - 65s 3ms/step - loss: 0.0872 - acc: 0.9651 - val_loss: 0.0965 - val_acc: 0.9622\n",
      "Epoch 13/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0840 - acc: 0.9668 - val_loss: 0.0972 - val_acc: 0.9620\n",
      "Epoch 14/100\n",
      "19682/19682 [==============================] - 63s 3ms/step - loss: 0.0804 - acc: 0.9675 - val_loss: 0.0955 - val_acc: 0.9608\n",
      "Epoch 15/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0775 - acc: 0.9684 - val_loss: 0.0973 - val_acc: 0.9601\n",
      "Epoch 16/100\n",
      "19682/19682 [==============================] - 65s 3ms/step - loss: 0.0739 - acc: 0.9700 - val_loss: 0.0955 - val_acc: 0.9622\n",
      "Epoch 17/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0727 - acc: 0.9704 - val_loss: 0.0962 - val_acc: 0.9622\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 18/100\n",
      "19682/19682 [==============================] - 65s 3ms/step - loss: 0.0685 - acc: 0.9724 - val_loss: 0.0964 - val_acc: 0.9614\n",
      "Epoch 19/100\n",
      "19682/19682 [==============================] - 68s 3ms/step - loss: 0.0666 - acc: 0.9728 - val_loss: 0.0953 - val_acc: 0.9617\n",
      "Epoch 20/100\n",
      "19682/19682 [==============================] - 61s 3ms/step - loss: 0.0666 - acc: 0.9728 - val_loss: 0.0982 - val_acc: 0.9613\n",
      "Epoch 21/100\n",
      "19682/19682 [==============================] - 62s 3ms/step - loss: 0.0647 - acc: 0.9738 - val_loss: 0.0955 - val_acc: 0.9625\n",
      "Epoch 22/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0639 - acc: 0.9736 - val_loss: 0.0959 - val_acc: 0.9629\n",
      "\n",
      "Epoch 00022: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 23/100\n",
      "19682/19682 [==============================] - 69s 4ms/step - loss: 0.0624 - acc: 0.9749 - val_loss: 0.0968 - val_acc: 0.9620\n",
      "Epoch 24/100\n",
      "19682/19682 [==============================] - 64s 3ms/step - loss: 0.0627 - acc: 0.9746 - val_loss: 0.0972 - val_acc: 0.9613\n",
      "Epoch 00024: early stopping\n",
      "Train on 19682 samples, validate on 4921 samples\n",
      "Epoch 1/100\n",
      "19682/19682 [==============================] - 74s 4ms/step - loss: 0.2701 - acc: 0.9235 - val_loss: 0.2187 - val_acc: 0.9400\n",
      "Epoch 2/100\n",
      "19682/19682 [==============================] - 76s 4ms/step - loss: 0.2088 - acc: 0.9399 - val_loss: 0.1935 - val_acc: 0.9399\n",
      "Epoch 3/100\n",
      "19682/19682 [==============================] - 74s 4ms/step - loss: 0.1804 - acc: 0.9406 - val_loss: 0.1625 - val_acc: 0.9429\n",
      "Epoch 4/100\n",
      "19682/19682 [==============================] - 78s 4ms/step - loss: 0.1533 - acc: 0.9450 - val_loss: 0.1421 - val_acc: 0.9479\n",
      "Epoch 5/100\n",
      "19682/19682 [==============================] - 78s 4ms/step - loss: 0.1323 - acc: 0.9499 - val_loss: 0.1249 - val_acc: 0.9531\n",
      "Epoch 6/100\n",
      "19682/19682 [==============================] - 67s 3ms/step - loss: 0.1190 - acc: 0.9549 - val_loss: 0.1170 - val_acc: 0.9553\n",
      "Epoch 7/100\n",
      "19682/19682 [==============================] - 68s 3ms/step - loss: 0.1102 - acc: 0.9574 - val_loss: 0.1113 - val_acc: 0.9559\n",
      "Epoch 8/100\n",
      "19682/19682 [==============================] - 73s 4ms/step - loss: 0.1035 - acc: 0.9594 - val_loss: 0.1087 - val_acc: 0.9568\n",
      "Epoch 9/100\n",
      "19682/19682 [==============================] - 65s 3ms/step - loss: 0.0988 - acc: 0.9615 - val_loss: 0.1027 - val_acc: 0.9596\n",
      "Epoch 10/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19682/19682 [==============================] - 69s 3ms/step - loss: 0.0936 - acc: 0.9631 - val_loss: 0.1024 - val_acc: 0.9592\n",
      "Epoch 11/100\n",
      "19682/19682 [==============================] - 67s 3ms/step - loss: 0.0921 - acc: 0.9639 - val_loss: 0.1001 - val_acc: 0.9609\n",
      "Epoch 12/100\n",
      "19682/19682 [==============================] - 66s 3ms/step - loss: 0.0875 - acc: 0.9652 - val_loss: 0.0982 - val_acc: 0.9599\n",
      "Epoch 13/100\n",
      "19682/19682 [==============================] - 69s 4ms/step - loss: 0.0848 - acc: 0.9663 - val_loss: 0.0986 - val_acc: 0.9599\n",
      "Epoch 14/100\n",
      "19682/19682 [==============================] - 66s 3ms/step - loss: 0.0821 - acc: 0.9668 - val_loss: 0.0953 - val_acc: 0.9620\n",
      "Epoch 15/100\n",
      "19682/19682 [==============================] - 78s 4ms/step - loss: 0.0784 - acc: 0.9685 - val_loss: 0.0961 - val_acc: 0.9610\n",
      "Epoch 16/100\n",
      "19682/19682 [==============================] - 66s 3ms/step - loss: 0.0760 - acc: 0.9688 - val_loss: 0.0939 - val_acc: 0.9622\n",
      "Epoch 17/100\n",
      "19682/19682 [==============================] - 69s 4ms/step - loss: 0.0739 - acc: 0.9704 - val_loss: 0.0945 - val_acc: 0.9622\n",
      "Epoch 18/100\n",
      "19682/19682 [==============================] - 74s 4ms/step - loss: 0.0725 - acc: 0.9710 - val_loss: 0.0946 - val_acc: 0.9613\n",
      "Epoch 19/100\n",
      "19682/19682 [==============================] - 72s 4ms/step - loss: 0.0688 - acc: 0.9718 - val_loss: 0.0947 - val_acc: 0.9606\n",
      "\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 20/100\n",
      "19682/19682 [==============================] - 67s 3ms/step - loss: 0.0662 - acc: 0.9732 - val_loss: 0.0945 - val_acc: 0.9612\n",
      "Epoch 21/100\n",
      "19682/19682 [==============================] - 72s 4ms/step - loss: 0.0654 - acc: 0.9734 - val_loss: 0.0941 - val_acc: 0.9615\n",
      "Epoch 00021: early stopping\n",
      "Train on 19683 samples, validate on 4920 samples\n",
      "Epoch 1/100\n",
      "19683/19683 [==============================] - 69s 3ms/step - loss: 0.2569 - acc: 0.9307 - val_loss: 0.2153 - val_acc: 0.9401\n",
      "Epoch 2/100\n",
      "19683/19683 [==============================] - 66s 3ms/step - loss: 0.2062 - acc: 0.9398 - val_loss: 0.1901 - val_acc: 0.9401\n",
      "Epoch 3/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.1791 - acc: 0.9409 - val_loss: 0.1608 - val_acc: 0.9435\n",
      "Epoch 4/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.1506 - acc: 0.9460 - val_loss: 0.1373 - val_acc: 0.9499\n",
      "Epoch 5/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.1304 - acc: 0.9509 - val_loss: 0.1204 - val_acc: 0.9547\n",
      "Epoch 6/100\n",
      "19683/19683 [==============================] - 66s 3ms/step - loss: 0.1179 - acc: 0.9552 - val_loss: 0.1173 - val_acc: 0.9553\n",
      "Epoch 7/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.1101 - acc: 0.9579 - val_loss: 0.1107 - val_acc: 0.9572\n",
      "Epoch 8/100\n",
      "19683/19683 [==============================] - 66s 3ms/step - loss: 0.1022 - acc: 0.9598 - val_loss: 0.1069 - val_acc: 0.9583\n",
      "Epoch 9/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.0987 - acc: 0.9611 - val_loss: 0.1079 - val_acc: 0.9573\n",
      "Epoch 10/100\n",
      "19683/19683 [==============================] - 67s 3ms/step - loss: 0.0945 - acc: 0.9627 - val_loss: 0.1032 - val_acc: 0.9594\n",
      "Epoch 11/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.0903 - acc: 0.9636 - val_loss: 0.1028 - val_acc: 0.9587\n",
      "Epoch 12/100\n",
      "19683/19683 [==============================] - 69s 3ms/step - loss: 0.0865 - acc: 0.9654 - val_loss: 0.0988 - val_acc: 0.9613\n",
      "Epoch 13/100\n",
      "19683/19683 [==============================] - 70s 4ms/step - loss: 0.0840 - acc: 0.9664 - val_loss: 0.0996 - val_acc: 0.9602\n",
      "Epoch 14/100\n",
      "19683/19683 [==============================] - 68s 3ms/step - loss: 0.0809 - acc: 0.9676 - val_loss: 0.0974 - val_acc: 0.9624\n",
      "Epoch 15/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.0783 - acc: 0.9680 - val_loss: 0.0965 - val_acc: 0.9623\n",
      "Epoch 16/100\n",
      "19683/19683 [==============================] - 73s 4ms/step - loss: 0.0757 - acc: 0.9690 - val_loss: 0.0966 - val_acc: 0.9619\n",
      "Epoch 17/100\n",
      "19683/19683 [==============================] - 68s 3ms/step - loss: 0.0734 - acc: 0.9705 - val_loss: 0.0974 - val_acc: 0.9604\n",
      "Epoch 18/100\n",
      "19683/19683 [==============================] - 68s 3ms/step - loss: 0.0714 - acc: 0.9709 - val_loss: 0.0965 - val_acc: 0.9624\n",
      "\n",
      "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 19/100\n",
      "19683/19683 [==============================] - 68s 3ms/step - loss: 0.0670 - acc: 0.9728 - val_loss: 0.0969 - val_acc: 0.9613\n",
      "Epoch 20/100\n",
      "19683/19683 [==============================] - 69s 3ms/step - loss: 0.0654 - acc: 0.9733 - val_loss: 0.0983 - val_acc: 0.9612\n",
      "Epoch 00020: early stopping\n",
      "Train on 19683 samples, validate on 4920 samples\n",
      "Epoch 1/100\n",
      "19683/19683 [==============================] - 76s 4ms/step - loss: 0.2560 - acc: 0.9307 - val_loss: 0.2207 - val_acc: 0.9381\n",
      "Epoch 2/100\n",
      "19683/19683 [==============================] - 62s 3ms/step - loss: 0.2029 - acc: 0.9403 - val_loss: 0.1914 - val_acc: 0.9386\n",
      "Epoch 3/100\n",
      "19683/19683 [==============================] - 66s 3ms/step - loss: 0.1728 - acc: 0.9424 - val_loss: 0.1617 - val_acc: 0.9428\n",
      "Epoch 4/100\n",
      "19683/19683 [==============================] - 75s 4ms/step - loss: 0.1486 - acc: 0.9471 - val_loss: 0.1405 - val_acc: 0.9478\n",
      "Epoch 5/100\n",
      "19683/19683 [==============================] - 67s 3ms/step - loss: 0.1317 - acc: 0.9507 - val_loss: 0.1279 - val_acc: 0.9512\n",
      "Epoch 6/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.1191 - acc: 0.9544 - val_loss: 0.1202 - val_acc: 0.9540\n",
      "Epoch 7/100\n",
      "19683/19683 [==============================] - 69s 4ms/step - loss: 0.1105 - acc: 0.9573 - val_loss: 0.1128 - val_acc: 0.9568\n",
      "Epoch 8/100\n",
      "19683/19683 [==============================] - 68s 3ms/step - loss: 0.1040 - acc: 0.9591 - val_loss: 0.1099 - val_acc: 0.9567\n",
      "Epoch 9/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.0979 - acc: 0.9611 - val_loss: 0.1043 - val_acc: 0.9597\n",
      "Epoch 10/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.0951 - acc: 0.9629 - val_loss: 0.1032 - val_acc: 0.9590\n",
      "Epoch 11/100\n",
      "19683/19683 [==============================] - 64s 3ms/step - loss: 0.0904 - acc: 0.9639 - val_loss: 0.0998 - val_acc: 0.9602\n",
      "Epoch 12/100\n",
      "19683/19683 [==============================] - 64s 3ms/step - loss: 0.0861 - acc: 0.9658 - val_loss: 0.0982 - val_acc: 0.9602\n",
      "Epoch 13/100\n",
      "19683/19683 [==============================] - 64s 3ms/step - loss: 0.0840 - acc: 0.9656 - val_loss: 0.0968 - val_acc: 0.9609\n",
      "Epoch 14/100\n",
      "19683/19683 [==============================] - 62s 3ms/step - loss: 0.0816 - acc: 0.9667 - val_loss: 0.0951 - val_acc: 0.9611\n",
      "Epoch 15/100\n",
      "19683/19683 [==============================] - 62s 3ms/step - loss: 0.0783 - acc: 0.9688 - val_loss: 0.0961 - val_acc: 0.9610\n",
      "Epoch 16/100\n",
      "19683/19683 [==============================] - 63s 3ms/step - loss: 0.0751 - acc: 0.9700 - val_loss: 0.0988 - val_acc: 0.9592\n",
      "Epoch 17/100\n",
      "19683/19683 [==============================] - 64s 3ms/step - loss: 0.0734 - acc: 0.9705 - val_loss: 0.0954 - val_acc: 0.9607\n",
      "\n",
      "Epoch 00017: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "Epoch 18/100\n",
      "19683/19683 [==============================] - 64s 3ms/step - loss: 0.0697 - acc: 0.9721 - val_loss: 0.0942 - val_acc: 0.9615\n",
      "Epoch 19/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.0683 - acc: 0.9726 - val_loss: 0.0944 - val_acc: 0.9620\n",
      "Epoch 20/100\n",
      "19683/19683 [==============================] - 63s 3ms/step - loss: 0.0678 - acc: 0.9734 - val_loss: 0.0933 - val_acc: 0.9624\n",
      "Epoch 21/100\n",
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.0661 - acc: 0.9734 - val_loss: 0.0937 - val_acc: 0.9625\n",
      "Epoch 22/100\n",
      "19683/19683 [==============================] - 63s 3ms/step - loss: 0.0655 - acc: 0.9738 - val_loss: 0.0930 - val_acc: 0.9627\n",
      "Epoch 23/100\n",
      "19683/19683 [==============================] - 70s 4ms/step - loss: 0.0650 - acc: 0.9740 - val_loss: 0.0935 - val_acc: 0.9618\n",
      "Epoch 24/100\n",
      "19683/19683 [==============================] - 60s 3ms/step - loss: 0.0636 - acc: 0.9744 - val_loss: 0.0951 - val_acc: 0.9615\n",
      "Epoch 25/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19683/19683 [==============================] - 65s 3ms/step - loss: 0.0632 - acc: 0.9745 - val_loss: 0.0942 - val_acc: 0.9621\n",
      "\n",
      "Epoch 00025: ReduceLROnPlateau reducing learning rate to 9.000000427477062e-05.\n",
      "Epoch 26/100\n",
      "19683/19683 [==============================] - 63s 3ms/step - loss: 0.0626 - acc: 0.9752 - val_loss: 0.0934 - val_acc: 0.9618\n",
      "Epoch 27/100\n",
      "19683/19683 [==============================] - 71s 4ms/step - loss: 0.0615 - acc: 0.9752 - val_loss: 0.0941 - val_acc: 0.9616\n",
      "Epoch 00027: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training Session\n",
    "\"\"\"\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "NUM_EPOCHS = 100\n",
    "FOLDER_NUM = 5\n",
    "\n",
    "# class_weights = class_weight.compute_class_weight('balanced', lb_sentiment.classes_, df['ground_truth_sentiment'])\n",
    "\n",
    "# splits = list(KFold(n_splits=FOLDER_NUM).split(train_text, aspect_vectors))\n",
    "splits = list(KFold(n_splits=FOLDER_NUM).split(train_text, aspect_vectors_train))\n",
    "\n",
    "# test_preds = np.zeros((test_text.shape[0]))\n",
    "folders = [i for i in range(FOLDER_NUM)]\n",
    "train_val_preds = np.zeros((aspect_vectors_train.shape))\n",
    "dev_preds = np.zeros((aspect_vectors_dev.shape))\n",
    "test_preds = np.zeros((aspect_vectors_test.shape))\n",
    "\n",
    "for folder in folders:\n",
    "    K.clear_session()\n",
    "    train_index, validate_index = splits[folder]\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, mode='min', verbose=1)\n",
    "#     checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=0, save_best_only=True)\n",
    "    model = build_model()\n",
    "    model.fit(x = train_text[train_index],\n",
    "              y = aspect_vectors_train[train_index],\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs = NUM_EPOCHS,\n",
    "              validation_data = (train_text[validate_index],\n",
    "                                 aspect_vectors_train[validate_index]),\n",
    "              callbacks = [reduce_lr, es]\n",
    "#               class_weight=class_weights\n",
    "             )\n",
    "    train_val_preds[validate_index] = model.predict(train_text[validate_index])\n",
    "    dev_preds += model.predict(dev_text)\n",
    "    test_preds += model.predict(test_text)\n",
    "dev_preds /= FOLDER_NUM\n",
    "test_preds /= FOLDER_NUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_training_predict.txt', train_val_preds, delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
