{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(fn, model_name):\n",
    "    vectors = []\n",
    "    with open(fn, 'r') as f:\n",
    "        for row in f.readlines():\n",
    "            vectors.append([float(each) for each in row.strip().split()])\n",
    "    cols = [model_name +'_access', model_name+'_costs', model_name+'_delays', model_name+'_errors', model_name+'_trusts']\n",
    "    vectors = pd.DataFrame(vectors, columns=cols)\n",
    "    return vectors\n",
    "\n",
    "def read_data_from_jaden(train_fn, dev_fn, test_fn, model_name):\n",
    "    train_data = read_data(train_fn, model_name)\n",
    "    dev_data = read_data(dev_fn, model_name)\n",
    "    test_data = read_data(test_fn, model_name)\n",
    "    \n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "def read_data_from_lya(train_fn, test_fn):\n",
    "    train_data = pd.read_csv(train_fn, header=0)\n",
    "    test_data = pd.read_csv(test_fn)\n",
    "    train_data.rename(columns={\"access\": \"BERT_access\", \"costs\": \"BERT_costs\", \"delays\":\"BERT_delays\", \"errors\":\"BERT_errors\", \"trusts\": \"BERT_trusts\"})\n",
    "    test_data.rename(columns={\"access\": \"BERT_access\", \"costs\": \"BERT_costs\", \"delays\":\"BERT_delays\", \"errors\":\"BERT_errors\", \"trusts\": \"BERT_trusts\"})\n",
    "\n",
    "    dev_data = test_data.iloc[:2500, :]\n",
    "    test_data = test_data.iloc[2500:, :]\n",
    "    \n",
    "    dev_data.reset_index(inplace=True, drop=True)\n",
    "    test_data.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return train_data, dev_data, test_data\n",
    "\n",
    "def read_prediction_for_training():\n",
    "    train_bert, dev_bert, test_bert = read_data_from_lya(\"BERT_Aspect_training_prediction_value.txt\", \n",
    "                                                         \"BERT_Aspect_test_prediction_value.txt\")\n",
    "    train_pooledrnn, dev_pooledrnn, test_pooledrnn = read_data_from_jaden(\"Pooled_RNN_Aspect_training_prediction_value.txt\", \n",
    "                                                                          \"Pooled_RNN_Aspect_dev_prediction_value.txt\", \n",
    "                                                                          \"Pooled_RNN_Aspect_test_prediction_value.txt\",\n",
    "                                                                          \"Pooled_RNN\")\n",
    "    train_attention, dev_attention, test_attention = read_data_from_jaden(\"Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_training_prediction_value.txt\", \n",
    "                                                                          \"Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_dev_prediction_value.txt\", \n",
    "                                                                          \"Pooled_RNN_ATTENTION_TEXT_CNN_Aspect_test_prediction_value.txt\", \n",
    "                                                                          \"Pooled_RNN_ATTENTION_TEXT_CNN\")  \n",
    "    train = pd.concat([train_bert, train_pooledrnn, train_attention], axis=1)\n",
    "    dev = pd.concat([dev_bert, dev_pooledrnn, dev_attention], axis=1)\n",
    "    test = pd.concat([test_bert, test_pooledrnn, test_attention], axis=1) \n",
    "    return train, dev, test\n",
    "\n",
    "\n",
    "import re\n",
    "def process(x):\n",
    "    ans = set()\n",
    "    for each in x[1:-1].split(','): \n",
    "        words = each.strip().strip('\"').strip(\"'\")\n",
    "        m = re.match('(\\w+)-', words)\n",
    "        if m is not None:\n",
    "            ans.add(m[1])\n",
    "#         else:\n",
    "#             ans.add(words)\n",
    "    return list(ans)\n",
    "\n",
    "\n",
    "\n",
    "def read_true_label():\n",
    "    df = pd.read_excel('../medical_sieve_training_set2_lower.xlsx')\n",
    "    df_t = pd.read_excel('../medical_sieve_test_set_lower.xlsx')\n",
    "    \n",
    "    df['ground_truth_aspect'] = df['ground_truth_subaspect'].apply(lambda x: process(x))\n",
    "    df_t['ground_truth_aspect'] = df_t['ground_truth_subaspect'].apply(lambda x: process(x))\n",
    "    df_dev = df_t[:2500]\n",
    "    df_test = df_t[2500:]\n",
    "    \n",
    "    mlb_aspect = MultiLabelBinarizer()\n",
    "    mlb_aspect.fit(df['ground_truth_aspect'])\n",
    "    aspect_vectors_train = mlb_aspect.transform(df['ground_truth_aspect'])\n",
    "    aspect_vectors_dev = mlb_aspect.transform(df_dev['ground_truth_aspect'])\n",
    "    aspect_vectors_test = mlb_aspect.transform(df_test['ground_truth_aspect'])\n",
    "    print(\"Unique types of compliant:\\n\",mlb_aspect.classes_)   \n",
    "    \n",
    "    return aspect_vectors_train, aspect_vectors_dev, aspect_vectors_test, mlb_aspect\n",
    "\n",
    "def read_corpus_prediction():\n",
    "    corpus_bert = pd.read_csv(\"BERT_Aspect_corpus_prediction_value.txt\", header=0)\n",
    "    corpus_pooledrnn = read_data(\"Pooled_RNN_Aspect_corpus_prediction_value.txt\", \"Pooled_RNN\")\n",
    "    corpus_attention = read_data(\"Pooled_RNN_Aspect_corpus_prediction_value.txt\", \"Pooled_RNN_ATTENTION_TEXT_CNN\")\n",
    "\n",
    "    corpus_data = pd.concat([corpus_bert, corpus_pooledrnn, corpus_attention], axis=1)\n",
    "    return corpus_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique types of compliant:\n",
      " ['access' 'costs' 'delays' 'errors' 'trusts']\n"
     ]
    }
   ],
   "source": [
    "train_pre, dev_pre, test_pre = read_prediction_for_training()\n",
    "train_true, dev_true, test_true, mlb_aspect = read_true_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input dim: 15, output dim: 5\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = train_pre.shape[1]\n",
    "OUTPUT_DIM = train_true.shape[1]\n",
    "print(\"input dim: {}, output dim: {}\".format(INPUT_DIM, OUTPUT_DIM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.layers import Input, Embedding, Concatenate,  Dense\n",
    "from keras.models import Model\n",
    "\n",
    "def build_model(input_dim, output_dim, verbose = True, compile = True):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    x = Dense(input_dim)(input_layer)\n",
    "    output_layer = Dense(output_dim, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    adam_optimizer = optimizers.Adam(lr=1e-3, decay=1e-6, clipvalue=5)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=adam_optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 43863 samples, validate on 2500 samples\n",
      "Epoch 1/100\n",
      "43863/43863 [==============================] - 1s 24us/step - loss: 0.5175 - acc: 0.9182 - val_loss: 0.3262 - val_acc: 0.9570\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.32620, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 2/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.2420 - acc: 0.9525 - val_loss: 0.1633 - val_acc: 0.9627\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.32620 to 0.16333, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 3/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.1414 - acc: 0.9635 - val_loss: 0.1137 - val_acc: 0.9671\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.16333 to 0.11374, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 4/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.1064 - acc: 0.9676 - val_loss: 0.0962 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.11374 to 0.09621, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 5/100\n",
      "43863/43863 [==============================] - 0s 8us/step - loss: 0.0944 - acc: 0.9690 - val_loss: 0.0898 - val_acc: 0.9698\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.09621 to 0.08982, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 6/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0904 - acc: 0.9691 - val_loss: 0.0876 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.08982 to 0.08764, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 7/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0888 - acc: 0.9691 - val_loss: 0.0869 - val_acc: 0.9696\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.08764 to 0.08694, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 8/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0881 - acc: 0.9690 - val_loss: 0.0867 - val_acc: 0.9694\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.08694 to 0.08672, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 9/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0877 - acc: 0.9691 - val_loss: 0.0859 - val_acc: 0.9697\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.08672 to 0.08591, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 10/100\n",
      "43863/43863 [==============================] - 0s 8us/step - loss: 0.0875 - acc: 0.9690 - val_loss: 0.0860 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.08591\n",
      "Epoch 11/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0872 - acc: 0.9690 - val_loss: 0.0860 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.08591\n",
      "Epoch 12/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0871 - acc: 0.9690 - val_loss: 0.0858 - val_acc: 0.9692\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.08591 to 0.08577, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 13/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0870 - acc: 0.9690 - val_loss: 0.0854 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00013: val_loss improved from 0.08577 to 0.08535, saving model to Ensemble_Model_256_Final.hdf5\n",
      "Epoch 14/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0869 - acc: 0.9690 - val_loss: 0.0857 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.08535\n",
      "Epoch 15/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0868 - acc: 0.9690 - val_loss: 0.0861 - val_acc: 0.9686\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.08535\n",
      "Epoch 16/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0868 - acc: 0.9690 - val_loss: 0.0859 - val_acc: 0.9690\n",
      "\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 0.0003000000142492354.\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.08535\n",
      "Epoch 17/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0867 - acc: 0.9690 - val_loss: 0.0859 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.08535\n",
      "Epoch 18/100\n",
      "43863/43863 [==============================] - 0s 9us/step - loss: 0.0867 - acc: 0.9690 - val_loss: 0.0856 - val_acc: 0.9689\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.08535\n",
      "Epoch 00018: early stopping\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Training Session\n",
    "\"\"\"\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "K.clear_session()\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, mode='min', verbose=1)\n",
    "checkpointer = ModelCheckpoint(filepath=\"Ensemble_Model_\"+str(BATCH_SIZE)+\"_Final.hdf5\", verbose=1, save_best_only=True)\n",
    "model = build_model(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "model.fit(x = train_pre,\n",
    "          y = train_true,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = NUM_EPOCHS,\n",
    "          validation_data = (dev_pre,\n",
    "                             dev_true),\n",
    "          callbacks = [reduce_lr, es, checkpointer]\n",
    "         )\n",
    "model.load_weights(\"Ensemble_Model_\"+str(BATCH_SIZE)+\"_Final.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def combinations(nums):\n",
    "    ans = [[]]\n",
    "    for row in nums:\n",
    "        curr = []\n",
    "        for combination in ans:\n",
    "            for element in row:\n",
    "                new_combination = copy.deepcopy(combination)\n",
    "                new_combination.append(element)\n",
    "                curr.append(new_combination)\n",
    "        ans = curr\n",
    "    return ans\n",
    "thresholds = [[0.2, 0.25, 0.3, 0.4, 0.45, 0.5, 0.55] for each in range(5)]\n",
    "\n",
    "thresholds_set = combinations(thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(dev_pre)\n",
    "aspect_vectors = dev_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold set: [0.4, 0.2, 0.25, 0.4, 0.3]\n",
      "Confusion Matrix for Each Aspect:\n",
      "============================================================\n",
      "[[[2215   96]\n",
      "  [  54  135]]\n",
      "\n",
      " [[2322   49]\n",
      "  [  29  100]]\n",
      "\n",
      " [[2380   34]\n",
      "  [  33   53]]\n",
      "\n",
      " [[2427   20]\n",
      "  [  22   31]]\n",
      "\n",
      " [[2391   28]\n",
      "  [  31   50]]]\n",
      "Result of Metrics for Evaluation:\n",
      "============================================================\n",
      "Hamming score: 0.96832\n",
      "Exact accuracy: 0.8468\n",
      "Fuzzy accuracy: 0.8468\n",
      "Exact accuracy (exclude negative): 0.6858736059479554\n",
      "Fuzzy accuracy (exclude negative): 0.6858736059479554\n",
      "Average F1 Score:  0.6400166060116974\n",
      "ROC AUC Score:  0.9677987766472353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def f1(matrix):\n",
    "    precision = matrix[1][1]*1.0 / (matrix[0][1] + matrix[1][1])\n",
    "    recall = matrix[1][1]*1.0 / (matrix[1][0] + matrix[1][1])\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "max_avg_f1 = 0\n",
    "max_hamming_score = 0\n",
    "max_exact_accuracy = 0\n",
    "max_fuzzy_accuracy = 0\n",
    "max_fuzzy_accuracy_pos = 0\n",
    "max_exact_accuracy_pos = 0\n",
    "max_avg_rocauc = 0\n",
    "max_confusion_matrix = None\n",
    "max_threshold_set = []\n",
    "thresholds_set = [[0.4, 0.2, 0.25, 0.4, 0.3]]\n",
    "for threshold_set in thresholds_set:\n",
    "    predict_softmax = np.zeros(aspect_vectors.shape, dtype=int)\n",
    "    for row_index, row in enumerate(val_preds):\n",
    "        for index, each in enumerate(row):\n",
    "            if each >= threshold_set[index]:\n",
    "                predict_softmax[row_index][index] = 1\n",
    "\n",
    "    hamming_score = 1 - hamming_loss(predict_softmax, aspect_vectors) \n",
    "    num_fuzzy_match = 0\n",
    "    num_fuzzy_match_pos = 0\n",
    "    num_exact_match_pos = 0\n",
    "    num_pos = 0\n",
    "    for true, pre in zip(mlb_aspect.inverse_transform(aspect_vectors), mlb_aspect.inverse_transform(predict_softmax)):\n",
    "        if len(true) != 0: \n",
    "            num_pos += 1\n",
    "        intersect = set(pre).intersection(set(true))\n",
    "        if (len(true)>0 and len(pre)>0 and len(intersect) > 0) or (len(true) == 0 and len(pre) == 0):\n",
    "            num_fuzzy_match += 1\n",
    "        if len(true)>0 and len(pre)>0 and len(intersect) > 0:\n",
    "            num_fuzzy_match_pos += 1\n",
    "        if len(true)>0 and len(pre)>0 and pre == true: \n",
    "            num_exact_match_pos += 1\n",
    "    fuzzy_accuracy = num_fuzzy_match*1.0/len(predict_softmax)\n",
    "    exact_accuracy = accuracy_score(predict_softmax, aspect_vectors)\n",
    "    fuzzy_accuracy_pos =  num_fuzzy_match_pos*1.0/num_pos\n",
    "    exact_accuracy_pos = num_exact_match_pos*1.0/num_pos\n",
    "\n",
    "\n",
    "    class_f1 = []\n",
    "    for aspect, confusion_matrix in zip(mlb_aspect.classes_, multilabel_confusion_matrix(aspect_vectors, predict_softmax)):\n",
    "#         print(aspect, ':',f1(confusion_matrix),'\\n', confusion_matrix, '\\n')\n",
    "        class_f1.append(f1(confusion_matrix))\n",
    "        \n",
    "    rocauc_score = roc_auc_score(aspect_vectors, val_preds, 'weighted')\n",
    "    if np.mean(class_f1) > max_avg_f1:\n",
    "        max_threshold_set = threshold_set\n",
    "        max_avg_f1 = max(max_avg_f1, np.mean(class_f1))\n",
    "        max_hamming_score = hamming_score\n",
    "        max_exact_accuracy = exact_accuracy\n",
    "        max_fuzzy_accuracy = fuzzy_accuracy \n",
    "        max_exact_accuracy_pos = exact_accuracy_pos\n",
    "        max_fuzzy_accuracy_pos = fuzzy_accuracy_pos\n",
    "        max_avg_rocauc = rocauc_score\n",
    "        max_confusion_matrix = multilabel_confusion_matrix(aspect_vectors, predict_softmax)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"threshold set:\", max_threshold_set)\n",
    "print(\"Confusion Matrix for Each Aspect:\\n\" + \"=\"*60)\n",
    "print(max_confusion_matrix)\n",
    "print(\"Result of Metrics for Evaluation:\\n\" + \"=\"*60)\n",
    "print(\"Hamming score:\", max_hamming_score)\n",
    "print(\"Exact accuracy:\", max_exact_accuracy)\n",
    "print(\"Fuzzy accuracy:\", max_fuzzy_accuracy)\n",
    "print(\"Exact accuracy (exclude negative):\", max_exact_accuracy_pos )\n",
    "print(\"Fuzzy accuracy (exclude negative):\", max_fuzzy_accuracy_pos)\n",
    "print(\"Average F1 Score: \", max_avg_f1)\n",
    "print(\"ROC AUC Score: \", max_avg_rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(test_pre)\n",
    "aspect_vectors = test_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold set: [0.4, 0.2, 0.25, 0.4, 0.3]\n",
      "Confusion Matrix for Each Aspect:\n",
      "============================================================\n",
      "[[[2201  132]\n",
      "  [  35  132]]\n",
      "\n",
      " [[2332   47]\n",
      "  [  37   84]]\n",
      "\n",
      " [[2378   27]\n",
      "  [  40   55]]\n",
      "\n",
      " [[2425   21]\n",
      "  [  20   34]]\n",
      "\n",
      " [[2389   41]\n",
      "  [  40   30]]]\n",
      "Result of Metrics for Evaluation:\n",
      "============================================================\n",
      "Hamming score: 0.9648\n",
      "Exact accuracy: 0.8284\n",
      "Fuzzy accuracy: 0.8284\n",
      "Exact accuracy (exclude negative): 0.6607495069033531\n",
      "Fuzzy accuracy (exclude negative): 0.6607495069033531\n",
      "Average F1 Score:  0.5900099442886633\n",
      "ROC AUC Score:  0.9660783638293943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def f1(matrix):\n",
    "    precision = matrix[1][1]*1.0 / (matrix[0][1] + matrix[1][1])\n",
    "    recall = matrix[1][1]*1.0 / (matrix[1][0] + matrix[1][1])\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "max_avg_f1 = 0\n",
    "max_hamming_score = 0\n",
    "max_exact_accuracy = 0\n",
    "max_fuzzy_accuracy = 0\n",
    "max_fuzzy_accuracy_pos = 0\n",
    "max_exact_accuracy_pos = 0\n",
    "max_avg_rocauc = 0\n",
    "max_confusion_matrix = None\n",
    "max_threshold_set = []\n",
    "thresholds_set = [[0.4, 0.2, 0.25, 0.4, 0.3]]\n",
    "for threshold_set in thresholds_set:\n",
    "    predict_softmax = np.zeros(aspect_vectors.shape, dtype=int)\n",
    "    for row_index, row in enumerate(val_preds):\n",
    "        for index, each in enumerate(row):\n",
    "            if each >= threshold_set[index]:\n",
    "                predict_softmax[row_index][index] = 1\n",
    "\n",
    "    hamming_score = 1 - hamming_loss(predict_softmax, aspect_vectors) \n",
    "    num_fuzzy_match = 0\n",
    "    num_fuzzy_match_pos = 0\n",
    "    num_exact_match_pos = 0\n",
    "    num_pos = 0\n",
    "    for true, pre in zip(mlb_aspect.inverse_transform(aspect_vectors), mlb_aspect.inverse_transform(predict_softmax)):\n",
    "        if len(true) != 0: \n",
    "            num_pos += 1\n",
    "        intersect = set(pre).intersection(set(true))\n",
    "        if (len(true)>0 and len(pre)>0 and len(intersect) > 0) or (len(true) == 0 and len(pre) == 0):\n",
    "            num_fuzzy_match += 1\n",
    "        if len(true)>0 and len(pre)>0 and len(intersect) > 0:\n",
    "            num_fuzzy_match_pos += 1\n",
    "        if len(true)>0 and len(pre)>0 and pre == true: \n",
    "            num_exact_match_pos += 1\n",
    "    fuzzy_accuracy = num_fuzzy_match*1.0/len(predict_softmax)\n",
    "    exact_accuracy = accuracy_score(predict_softmax, aspect_vectors)\n",
    "    fuzzy_accuracy_pos =  num_fuzzy_match_pos*1.0/num_pos\n",
    "    exact_accuracy_pos = num_exact_match_pos*1.0/num_pos\n",
    "\n",
    "\n",
    "    class_f1 = []\n",
    "    for aspect, confusion_matrix in zip(mlb_aspect.classes_, multilabel_confusion_matrix(aspect_vectors, predict_softmax)):\n",
    "#         print(aspect, ':',f1(confusion_matrix),'\\n', confusion_matrix, '\\n')\n",
    "        class_f1.append(f1(confusion_matrix))\n",
    "        \n",
    "    rocauc_score = roc_auc_score(aspect_vectors, val_preds, 'weighted')\n",
    "    if np.mean(class_f1) > max_avg_f1:\n",
    "        max_threshold_set = threshold_set\n",
    "        max_avg_f1 = max(max_avg_f1, np.mean(class_f1))\n",
    "        max_hamming_score = hamming_score\n",
    "        max_exact_accuracy = exact_accuracy\n",
    "        max_fuzzy_accuracy = fuzzy_accuracy \n",
    "        max_exact_accuracy_pos = exact_accuracy_pos\n",
    "        max_fuzzy_accuracy_pos = fuzzy_accuracy_pos\n",
    "        max_avg_rocauc = rocauc_score\n",
    "        max_confusion_matrix = multilabel_confusion_matrix(aspect_vectors, predict_softmax)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"threshold set:\", max_threshold_set)\n",
    "print(\"Confusion Matrix for Each Aspect:\\n\" + \"=\"*60)\n",
    "print(max_confusion_matrix)\n",
    "print(\"Result of Metrics for Evaluation:\\n\" + \"=\"*60)\n",
    "print(\"Hamming score:\", max_hamming_score)\n",
    "print(\"Exact accuracy:\", max_exact_accuracy)\n",
    "print(\"Fuzzy accuracy:\", max_fuzzy_accuracy)\n",
    "print(\"Exact accuracy (exclude negative):\", max_exact_accuracy_pos )\n",
    "print(\"Fuzzy accuracy (exclude negative):\", max_fuzzy_accuracy_pos)\n",
    "print(\"Average F1 Score: \", max_avg_f1)\n",
    "print(\"ROC AUC Score: \", max_avg_rocauc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = model.predict(train_pre)\n",
    "aspect_vectors = train_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threshold set: [0.4, 0.2, 0.25, 0.4, 0.3]\n",
      "Confusion Matrix for Each Aspect:\n",
      "============================================================\n",
      "[[[40458   588]\n",
      "  [  577  2240]]\n",
      "\n",
      " [[40321   864]\n",
      "  [  492  2186]]\n",
      "\n",
      " [[41230   666]\n",
      "  [  615  1352]]\n",
      "\n",
      " [[41758   447]\n",
      "  [  811   847]]\n",
      "\n",
      " [[40875   886]\n",
      "  [ 1027  1075]]]\n",
      "Result of Metrics for Evaluation:\n",
      "============================================================\n",
      "Hamming score: 0.9682055490960491\n",
      "Exact accuracy: 0.8466361170006611\n",
      "Fuzzy accuracy: 0.8480496090098717\n",
      "Exact accuracy (exclude negative): 0.6837392550143266\n",
      "Fuzzy accuracy (exclude negative): 0.6892908309455588\n",
      "Average F1 Score:  0.6676898506050032\n",
      "ROC AUC Score:  0.9706521611491519\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def f1(matrix):\n",
    "    precision = matrix[1][1]*1.0 / (matrix[0][1] + matrix[1][1])\n",
    "    recall = matrix[1][1]*1.0 / (matrix[1][0] + matrix[1][1])\n",
    "    return 2*((precision*recall)/(precision+recall))\n",
    "\n",
    "max_avg_f1 = 0\n",
    "max_hamming_score = 0\n",
    "max_exact_accuracy = 0\n",
    "max_fuzzy_accuracy = 0\n",
    "max_fuzzy_accuracy_pos = 0\n",
    "max_exact_accuracy_pos = 0\n",
    "max_avg_rocauc = 0\n",
    "max_confusion_matrix = None\n",
    "max_threshold_set = []\n",
    "thresholds_set = [[0.4, 0.2, 0.25, 0.4, 0.3]]\n",
    "\n",
    "for threshold_set in thresholds_set:\n",
    "    predict_softmax = np.zeros(aspect_vectors.shape, dtype=int)\n",
    "    for row_index, row in enumerate(val_preds):\n",
    "        for index, each in enumerate(row):\n",
    "            if each >= threshold_set[index]:\n",
    "                predict_softmax[row_index][index] = 1\n",
    "\n",
    "    hamming_score = 1 - hamming_loss(predict_softmax, aspect_vectors) \n",
    "    num_fuzzy_match = 0\n",
    "    num_fuzzy_match_pos = 0\n",
    "    num_exact_match_pos = 0\n",
    "    num_pos = 0\n",
    "    for true, pre in zip(mlb_aspect.inverse_transform(aspect_vectors), mlb_aspect.inverse_transform(predict_softmax)):\n",
    "        if len(true) != 0: \n",
    "            num_pos += 1\n",
    "        intersect = set(pre).intersection(set(true))\n",
    "        if (len(true)>0 and len(pre)>0 and len(intersect) > 0) or (len(true) == 0 and len(pre) == 0):\n",
    "            num_fuzzy_match += 1\n",
    "        if len(true)>0 and len(pre)>0 and len(intersect) > 0:\n",
    "            num_fuzzy_match_pos += 1\n",
    "        if len(true)>0 and len(pre)>0 and pre == true: \n",
    "            num_exact_match_pos += 1\n",
    "    fuzzy_accuracy = num_fuzzy_match*1.0/len(predict_softmax)\n",
    "    exact_accuracy = accuracy_score(predict_softmax, aspect_vectors)\n",
    "    fuzzy_accuracy_pos =  num_fuzzy_match_pos*1.0/num_pos\n",
    "    exact_accuracy_pos = num_exact_match_pos*1.0/num_pos\n",
    "\n",
    "\n",
    "    class_f1 = []\n",
    "    for aspect, confusion_matrix in zip(mlb_aspect.classes_, multilabel_confusion_matrix(aspect_vectors, predict_softmax)):\n",
    "#         print(aspect, ':',f1(confusion_matrix),'\\n', confusion_matrix, '\\n')\n",
    "        class_f1.append(f1(confusion_matrix))\n",
    "        \n",
    "    rocauc_score = roc_auc_score(aspect_vectors, val_preds, 'weighted')\n",
    "    if np.mean(class_f1) > max_avg_f1:\n",
    "        max_threshold_set = threshold_set\n",
    "        max_avg_f1 = max(max_avg_f1, np.mean(class_f1))\n",
    "        max_hamming_score = hamming_score\n",
    "        max_exact_accuracy = exact_accuracy\n",
    "        max_fuzzy_accuracy = fuzzy_accuracy \n",
    "        max_exact_accuracy_pos = exact_accuracy_pos\n",
    "        max_fuzzy_accuracy_pos = fuzzy_accuracy_pos\n",
    "        max_avg_rocauc = rocauc_score\n",
    "        max_confusion_matrix = multilabel_confusion_matrix(aspect_vectors, predict_softmax)\n",
    "        \n",
    "        \n",
    "\n",
    "print(\"threshold set:\", max_threshold_set)\n",
    "print(\"Confusion Matrix for Each Aspect:\\n\" + \"=\"*60)\n",
    "print(max_confusion_matrix)\n",
    "print(\"Result of Metrics for Evaluation:\\n\" + \"=\"*60)\n",
    "print(\"Hamming score:\", max_hamming_score)\n",
    "print(\"Exact accuracy:\", max_exact_accuracy)\n",
    "print(\"Fuzzy accuracy:\", max_fuzzy_accuracy)\n",
    "print(\"Exact accuracy (exclude negative):\", max_exact_accuracy_pos )\n",
    "print(\"Fuzzy accuracy (exclude negative):\", max_fuzzy_accuracy_pos)\n",
    "print(\"Average F1 Score: \", max_avg_f1)\n",
    "print(\"ROC AUC Score: \", max_avg_rocauc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction on all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 15)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 5)                 80        \n",
      "=================================================================\n",
      "Total params: 320\n",
      "Trainable params: 320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 256\n",
    "corpus_data = read_corpus_prediction()\n",
    "model = build_model(input_dim=INPUT_DIM, output_dim=OUTPUT_DIM)\n",
    "model.load_weights(\"Ensemble_Model_\"+str(BATCH_SIZE)+\"_Final.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_val_preds = model.predict(corpus_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('Ensemble_Aspect_corpus_prediction_value.txt', corpus_val_preds, delimiter=' ')\n",
    "threshold_set = [0.4, 0.2, 0.25, 0.4, 0.3]\n",
    "predict_softmax = np.zeros((corpus_data.shape[0], corpus_val_preds.shape[1]), dtype=int)\n",
    "for row_index, row in enumerate(corpus_val_preds):\n",
    "    for index, each in enumerate(row):\n",
    "        if each >= threshold_set[index]:\n",
    "            predict_softmax[row_index][index] = 1\n",
    "np.savetxt('Ensemble_Aspect_corpus_prediction.txt', predict_softmax.astype(int), delimiter=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
