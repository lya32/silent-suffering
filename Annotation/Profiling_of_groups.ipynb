{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(lines, filename):\n",
    "    # extract and store the start and end index of entity in .txt file\n",
    "    entity_name = {\"UNC\", \"NEG\", \"SEV\", \"COU\", \"CON\", \"drug\",\"treatment\", \"problem\", \"test\"}\n",
    "    annotation = {}\n",
    "    semantic_p = re.compile('semantic=(.*)')\n",
    "    entity_p = re.compile('ne=(.*)')\n",
    "    for line in lines:\n",
    "        each = line.split(\"\\t\") \n",
    "        if each[0] == \"NamedEntity\" and semantic_p.findall(each[3])[0] in entity_name:\n",
    "            entity = entity_p.findall(each[-1])[0]\n",
    "            annotation[each[1] + '-' + each[2]] = {\"Semantic\": semantic_p.findall(each[3])[0], \"Entity\":entity.strip()}\n",
    "    \n",
    "    # extract info from .xmi file with the help of .txt file\n",
    "    relation_name = {\"CON_Of\", \"COU_Of\", \"NEG_Of\", \"SEV_Of\", \"UNC_Of\"}\n",
    "    entity_dict = {}\n",
    "    relation_dict = {\"NEG_Of\":[], \"negation_count\": 0, \n",
    "                     \"SEV_Of\":[], \"severity_count\":0,\n",
    "                     \"UNC_Of\":[], \"uncertainty_count\": 0, \n",
    "                     \"CON_Of\":[], \n",
    "                     \"COU_Of\":[], \"dynamic_count\": 0,}\n",
    "    tree = ET.parse(filename[:-4] + '.xmi')\n",
    "    root = tree.getroot()\n",
    "    for child in root:\n",
    "        if 'ClampNameEntityUIMA' in child.tag and child.attrib['semanticTag'] in entity_name:\n",
    "            entity_dict[child.attrib['{http://www.omg.org/XMI}id']] = {\"begin-end\": child.attrib['begin'] + '-' + child.attrib['end'], \"Semantic\": child.attrib['semanticTag']}\n",
    "            \n",
    "        if 'ClampRelationUIMA' in child.tag and child.attrib['semanticTag'] in relation_name:\n",
    "            fromEntity = annotation[entity_dict[child.attrib['entTo']][\"begin-end\"]][\"Entity\"]\n",
    "            toEntity = annotation[entity_dict[child.attrib['entFrom']][\"begin-end\"]][\"Entity\"]\n",
    "            if child.attrib['semanticTag'] == \"CON_Of\":\n",
    "                relation = fromEntity + ' something ->' + toEntity\n",
    "            else:\n",
    "                relation = fromEntity + '->' + toEntity\n",
    "            relation_dict[child.attrib['semanticTag']].append(relation)\n",
    "    relation_dict[\"severity_count\"] = 1 if (len(relation_dict[\"SEV_Of\"]) > 0) else 0 \n",
    "    relation_dict[\"negation_count\"] = 1 if (len(relation_dict[\"NEG_Of\"]) > 0) else 0 \n",
    "    relation_dict[\"uncertainty_count\"] = 1 if len(relation_dict[\"UNC_Of\"]) else 0\n",
    "    relation_dict[\"dynamic_count\"] = 1 if len(relation_dict[\"COU_Of\"]) else 0\n",
    "    return relation_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "whole_dict = {}\n",
    "raw_text_path = \"/Volumes/E/ClampCmd_1.5.1/input\"\n",
    "entity_annotation_path = \"/Volumes/E/ClampCmd_1.5.1/output\"\n",
    "\n",
    "\n",
    "for files in os.listdir(raw_text_path):\n",
    "    if '.' not in files:\n",
    "        print(files)\n",
    "        for filename in os.listdir(raw_text_path + '/' + files):\n",
    "            if filename[-4:] == \".txt\":\n",
    "                text_f = open(raw_text_path + '/' + files + '/' + filename, 'r')\n",
    "                endIndex = re.search('-',filename).span()[0]\n",
    "                group = filename[:endIndex]\n",
    "                if not group in whole_dict:\n",
    "                    whole_dict[group] = {'annotation': {'NEG': [], 'CON':[], 'UNC':[], 'COU': [], 'treatment': []}, 'entry':{}}\n",
    "                    whole_dict[group]['entry'][filename[:-4]] = {\"text\": text_f.readline()}\n",
    "                else:\n",
    "                    whole_dict[group]['entry'][filename[:-4]] = {\"text\": text_f.readline()}\n",
    "                text_f.close()\n",
    "\n",
    "                anno_f = open(entity_annotation_path + '/' + filename[0] + \"/\" + filename, 'r')\n",
    "                anotattion = parse(anno_f.readlines(), (entity_annotation_path + '/' + filename[0] + \"/\" + filename))\n",
    "                whole_dict[group]['entry'][filename[:-4]].update(anotattion)\n",
    "                if anotattion['NEG_Of'] != []:\n",
    "                    for each in anotattion['NEG_Of']:\n",
    "                        whole_dict[group]['annotation']['NEG'].append(each)\n",
    "                if anotattion['CON_Of'] != []:\n",
    "                    for each in anotattion['CON_Of']:\n",
    "                        whole_dict[group]['annotation']['CON'].append(each)\n",
    "                if anotattion['UNC_Of'] != []:\n",
    "                    for each in anotattion['UNC_Of']:\n",
    "                        whole_dict[group]['annotation']['UNC'].append(each)\n",
    "                if anotattion['COU_Of'] != []:\n",
    "                    for each in anotattion['COU_Of']:\n",
    "                        whole_dict[group]['annotation']['COU'].append(each)\n",
    "                anno_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('patient_text_profile.json', 'w') as f:\n",
    "    json.dump(whole_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load stats data\n",
    "stat = {}\n",
    "f = open(\"group_profiling.txt\",'r')\n",
    "for each in f.readlines():\n",
    "    data = json.loads(each)\n",
    "    stat[data['group']] = {'members': data['member'], 'post': data['post']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load unique poster stats data\n",
    "stat_unique = {}\n",
    "with open(\"unique_poster.txt\") as json_file:\n",
    "    data = json.load(json_file)\n",
    "for key, value in data.items():\n",
    "    stat_unique[key] = {'unique_poster': value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge stats and all ratio metrics\n",
    "profiling = {}\n",
    "for group, files in whole_dict.items():\n",
    "    negation_count = 0\n",
    "    uncertainty_count = 0\n",
    "    severity_count = 0\n",
    "    dynamic_count = 0\n",
    "    count = Counter()\n",
    "    for each in files['entry'].values():\n",
    "        negation_count += each['negation_count']\n",
    "        uncertainty_count += each['uncertainty_count']\n",
    "        severity_count += each['severity_count']\n",
    "        dynamic_count += each['dynamic_count']\n",
    "        for word in each['SEV_Of']:\n",
    "            p = re.compile('(.*)->\\w+')\n",
    "            count[p.findall(word)[0]] += 1\n",
    "    profiling[group] = {}\n",
    "    profiling[group].update(stat[group])\n",
    "    profiling[group].update(stat_unique[group])\n",
    "    profiling[group]['severity_top'] = count.most_common(1)[0][0]\n",
    "    profiling[group]['negation_ratio'] = negation_count/profiling[group]['post']\n",
    "    profiling[group]['uncertainty_ratio'] = uncertainty_count/profiling[group]['post']\n",
    "    profiling[group]['severity_ratio'] = severity_count/profiling[group]['post']\n",
    "    profiling[group]['dynamic_ratio'] = dynamic_count/profiling[group]['post']\n",
    "    profiling[group].update(whole_dict[group]['annotation'])\n",
    "df = pd.read_json(json.dumps(profiling), orient='index')\n",
    "df = df[['members', 'unique_poster','post','severity_top','severity_ratio', 'uncertainty_ratio', 'dynamic_ratio', 'negation_ratio', 'UNC', 'COU', 'CON', 'NEG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical profiling of groups in terms of Linguistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>members</th>\n",
       "      <th>unique_poster</th>\n",
       "      <th>post</th>\n",
       "      <th>severity_top</th>\n",
       "      <th>severity_ratio</th>\n",
       "      <th>uncertainty_ratio</th>\n",
       "      <th>dynamic_ratio</th>\n",
       "      <th>negation_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>6468</td>\n",
       "      <td>4950</td>\n",
       "      <td>40915</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.044287</td>\n",
       "      <td>0.099401</td>\n",
       "      <td>0.024612</td>\n",
       "      <td>0.080288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Abscess_Non_dental</td>\n",
       "      <td>409</td>\n",
       "      <td>223</td>\n",
       "      <td>1095</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.033790</td>\n",
       "      <td>0.109589</td>\n",
       "      <td>0.027397</td>\n",
       "      <td>0.115068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Accidents_and_Injuries</td>\n",
       "      <td>1801</td>\n",
       "      <td>1170</td>\n",
       "      <td>7246</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.028982</td>\n",
       "      <td>0.060033</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.067486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACE_Inhibitors</td>\n",
       "      <td>229</td>\n",
       "      <td>82</td>\n",
       "      <td>352</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.042614</td>\n",
       "      <td>0.068182</td>\n",
       "      <td>0.019886</td>\n",
       "      <td>0.065341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Acne</td>\n",
       "      <td>654</td>\n",
       "      <td>273</td>\n",
       "      <td>1127</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.047028</td>\n",
       "      <td>0.044366</td>\n",
       "      <td>0.023957</td>\n",
       "      <td>0.047915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Adrenal_Disorders</td>\n",
       "      <td>501</td>\n",
       "      <td>324</td>\n",
       "      <td>2204</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.032214</td>\n",
       "      <td>0.104809</td>\n",
       "      <td>0.028584</td>\n",
       "      <td>0.077586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alcohol_Consumption</td>\n",
       "      <td>1529</td>\n",
       "      <td>1191</td>\n",
       "      <td>32247</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.010265</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>0.008652</td>\n",
       "      <td>0.036593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alendronic_Acid</td>\n",
       "      <td>278</td>\n",
       "      <td>138</td>\n",
       "      <td>1522</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.026938</td>\n",
       "      <td>0.052562</td>\n",
       "      <td>0.023653</td>\n",
       "      <td>0.061761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Allergic_Disorders</td>\n",
       "      <td>646</td>\n",
       "      <td>335</td>\n",
       "      <td>1472</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.030571</td>\n",
       "      <td>0.104620</td>\n",
       "      <td>0.034647</td>\n",
       "      <td>0.080163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alopecia_and_Hair_Disorders</td>\n",
       "      <td>528</td>\n",
       "      <td>280</td>\n",
       "      <td>1200</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.064167</td>\n",
       "      <td>0.018333</td>\n",
       "      <td>0.052500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Alternative_and_Complementary_Medicine</td>\n",
       "      <td>1087</td>\n",
       "      <td>817</td>\n",
       "      <td>8792</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.020701</td>\n",
       "      <td>0.043790</td>\n",
       "      <td>0.023999</td>\n",
       "      <td>0.047202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Alzheimers_Disease</td>\n",
       "      <td>225</td>\n",
       "      <td>98</td>\n",
       "      <td>357</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.016807</td>\n",
       "      <td>0.075630</td>\n",
       "      <td>0.042017</td>\n",
       "      <td>0.033613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Amitriptyline</td>\n",
       "      <td>741</td>\n",
       "      <td>626</td>\n",
       "      <td>5335</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.024930</td>\n",
       "      <td>0.044049</td>\n",
       "      <td>0.020806</td>\n",
       "      <td>0.052671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Amlodipine</td>\n",
       "      <td>852</td>\n",
       "      <td>745</td>\n",
       "      <td>5293</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.031551</td>\n",
       "      <td>0.055167</td>\n",
       "      <td>0.024561</td>\n",
       "      <td>0.073115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Anaemia</td>\n",
       "      <td>756</td>\n",
       "      <td>450</td>\n",
       "      <td>2607</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.035290</td>\n",
       "      <td>0.075182</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.056387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Anal_Fissure_and_Proctalgia</td>\n",
       "      <td>1090</td>\n",
       "      <td>960</td>\n",
       "      <td>7216</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.027162</td>\n",
       "      <td>0.051829</td>\n",
       "      <td>0.030072</td>\n",
       "      <td>0.094512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Angina</td>\n",
       "      <td>364</td>\n",
       "      <td>257</td>\n",
       "      <td>2667</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.065992</td>\n",
       "      <td>0.104987</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>0.091489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Angiotensin_II_Receptor_Blockers</td>\n",
       "      <td>280</td>\n",
       "      <td>184</td>\n",
       "      <td>1193</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.024308</td>\n",
       "      <td>0.054484</td>\n",
       "      <td>0.029338</td>\n",
       "      <td>0.062867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ankle_Problems</td>\n",
       "      <td>1924</td>\n",
       "      <td>1994</td>\n",
       "      <td>26540</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.036812</td>\n",
       "      <td>0.019329</td>\n",
       "      <td>0.059495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ankylosing_Spondylitis</td>\n",
       "      <td>606</td>\n",
       "      <td>389</td>\n",
       "      <td>3827</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.070551</td>\n",
       "      <td>0.040763</td>\n",
       "      <td>0.076823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Antibiotics</td>\n",
       "      <td>674</td>\n",
       "      <td>260</td>\n",
       "      <td>1009</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.052527</td>\n",
       "      <td>0.052527</td>\n",
       "      <td>0.025768</td>\n",
       "      <td>0.072349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Anticoagulants</td>\n",
       "      <td>352</td>\n",
       "      <td>204</td>\n",
       "      <td>1699</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.017657</td>\n",
       "      <td>0.036492</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.048852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Anxiety_Disorders</td>\n",
       "      <td>14795</td>\n",
       "      <td>12780</td>\n",
       "      <td>196049</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.020689</td>\n",
       "      <td>0.047131</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.037649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Aortic_Aneurysm_and_Dissection</td>\n",
       "      <td>363</td>\n",
       "      <td>263</td>\n",
       "      <td>2504</td>\n",
       "      <td>acute</td>\n",
       "      <td>0.026358</td>\n",
       "      <td>0.058706</td>\n",
       "      <td>0.015176</td>\n",
       "      <td>0.077077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Appendicitis</td>\n",
       "      <td>220</td>\n",
       "      <td>88</td>\n",
       "      <td>378</td>\n",
       "      <td>sharp</td>\n",
       "      <td>0.082011</td>\n",
       "      <td>0.084656</td>\n",
       "      <td>0.026455</td>\n",
       "      <td>0.097884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Arthritis</td>\n",
       "      <td>724</td>\n",
       "      <td>403</td>\n",
       "      <td>2480</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.031048</td>\n",
       "      <td>0.058468</td>\n",
       "      <td>0.027419</td>\n",
       "      <td>0.056048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Arthroscopy</td>\n",
       "      <td>350</td>\n",
       "      <td>172</td>\n",
       "      <td>1133</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>0.034422</td>\n",
       "      <td>0.010591</td>\n",
       "      <td>0.065313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Asthma</td>\n",
       "      <td>629</td>\n",
       "      <td>304</td>\n",
       "      <td>1771</td>\n",
       "      <td>mild</td>\n",
       "      <td>0.054207</td>\n",
       "      <td>0.102202</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>0.083004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Atrial_Fibrillation_And_Flutter</td>\n",
       "      <td>820</td>\n",
       "      <td>616</td>\n",
       "      <td>10065</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>0.056234</td>\n",
       "      <td>0.016990</td>\n",
       "      <td>0.068356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Atrophic_Vaginitis</td>\n",
       "      <td>572</td>\n",
       "      <td>435</td>\n",
       "      <td>9893</td>\n",
       "      <td>severe</td>\n",
       "      <td>0.014151</td>\n",
       "      <td>0.040938</td>\n",
       "      <td>0.013949</td>\n",
       "      <td>0.050339</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Unnamed: 0  members  unique_poster    post  \\\n",
       "0                      Abdominal_Disorders     6468           4950   40915   \n",
       "1                       Abscess_Non_dental      409            223    1095   \n",
       "2                   Accidents_and_Injuries     1801           1170    7246   \n",
       "3                           ACE_Inhibitors      229             82     352   \n",
       "4                                     Acne      654            273    1127   \n",
       "5                        Adrenal_Disorders      501            324    2204   \n",
       "6                      Alcohol_Consumption     1529           1191   32247   \n",
       "7                          Alendronic_Acid      278            138    1522   \n",
       "8                       Allergic_Disorders      646            335    1472   \n",
       "9              Alopecia_and_Hair_Disorders      528            280    1200   \n",
       "10  Alternative_and_Complementary_Medicine     1087            817    8792   \n",
       "11                      Alzheimers_Disease      225             98     357   \n",
       "12                           Amitriptyline      741            626    5335   \n",
       "13                              Amlodipine      852            745    5293   \n",
       "14                                 Anaemia      756            450    2607   \n",
       "15             Anal_Fissure_and_Proctalgia     1090            960    7216   \n",
       "16                                  Angina      364            257    2667   \n",
       "17        Angiotensin_II_Receptor_Blockers      280            184    1193   \n",
       "18                          Ankle_Problems     1924           1994   26540   \n",
       "19                  Ankylosing_Spondylitis      606            389    3827   \n",
       "20                             Antibiotics      674            260    1009   \n",
       "21                          Anticoagulants      352            204    1699   \n",
       "22                       Anxiety_Disorders    14795          12780  196049   \n",
       "23          Aortic_Aneurysm_and_Dissection      363            263    2504   \n",
       "24                            Appendicitis      220             88     378   \n",
       "25                               Arthritis      724            403    2480   \n",
       "26                             Arthroscopy      350            172    1133   \n",
       "27                                  Asthma      629            304    1771   \n",
       "28         Atrial_Fibrillation_And_Flutter      820            616   10065   \n",
       "29                      Atrophic_Vaginitis      572            435    9893   \n",
       "\n",
       "   severity_top  severity_ratio  uncertainty_ratio  dynamic_ratio  \\\n",
       "0        severe        0.044287           0.099401       0.024612   \n",
       "1        severe        0.033790           0.109589       0.027397   \n",
       "2        severe        0.028982           0.060033       0.025393   \n",
       "3        severe        0.042614           0.068182       0.019886   \n",
       "4        severe        0.047028           0.044366       0.023957   \n",
       "5        severe        0.032214           0.104809       0.028584   \n",
       "6        severe        0.010265           0.025770       0.008652   \n",
       "7        severe        0.026938           0.052562       0.023653   \n",
       "8        severe        0.030571           0.104620       0.034647   \n",
       "9        severe        0.015000           0.064167       0.018333   \n",
       "10       severe        0.020701           0.043790       0.023999   \n",
       "11       severe        0.016807           0.075630       0.042017   \n",
       "12       severe        0.024930           0.044049       0.020806   \n",
       "13       severe        0.031551           0.055167       0.024561   \n",
       "14       severe        0.035290           0.075182       0.030303   \n",
       "15       severe        0.027162           0.051829       0.030072   \n",
       "16       severe        0.065992           0.104987       0.036745   \n",
       "17       severe        0.024308           0.054484       0.029338   \n",
       "18       severe        0.021439           0.036812       0.019329   \n",
       "19       severe        0.034230           0.070551       0.040763   \n",
       "20       severe        0.052527           0.052527       0.025768   \n",
       "21       severe        0.017657           0.036492       0.020600   \n",
       "22       severe        0.020689           0.047131       0.013808   \n",
       "23        acute        0.026358           0.058706       0.015176   \n",
       "24        sharp        0.082011           0.084656       0.026455   \n",
       "25       severe        0.031048           0.058468       0.027419   \n",
       "26       severe        0.034422           0.034422       0.010591   \n",
       "27         mild        0.054207           0.102202       0.040090   \n",
       "28       severe        0.018082           0.056234       0.016990   \n",
       "29       severe        0.014151           0.040938       0.013949   \n",
       "\n",
       "    negation_ratio  \n",
       "0         0.080288  \n",
       "1         0.115068  \n",
       "2         0.067486  \n",
       "3         0.065341  \n",
       "4         0.047915  \n",
       "5         0.077586  \n",
       "6         0.036593  \n",
       "7         0.061761  \n",
       "8         0.080163  \n",
       "9         0.052500  \n",
       "10        0.047202  \n",
       "11        0.033613  \n",
       "12        0.052671  \n",
       "13        0.073115  \n",
       "14        0.056387  \n",
       "15        0.094512  \n",
       "16        0.091489  \n",
       "17        0.062867  \n",
       "18        0.059495  \n",
       "19        0.076823  \n",
       "20        0.072349  \n",
       "21        0.048852  \n",
       "22        0.037649  \n",
       "23        0.077077  \n",
       "24        0.097884  \n",
       "25        0.056048  \n",
       "26        0.065313  \n",
       "27        0.083004  \n",
       "28        0.068356  \n",
       "29        0.050339  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('groups_profiling.csv')\n",
    "df.head(30)\n",
    "\n",
    "# df.sort_values(by=['dynamic_ratio' ], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('groups_profiling.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WordCould of Specified Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "stopwords = set(STOPWORDS)\n",
    "stopwords.update([\"problems\",\"problem\", \"not\", 'without', 'never','pain', 'symptom', 'symptoms', 'issue', 'issues', 'increase', 'improve', 'worse', 'reduce', 'low', 'slightly', 'severe', 'lower', 'bad', 'damage', 'drop', 'less', 'much', ''])\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def collect(whole_dict, target_groups, concept):\n",
    "    text = \"\"\n",
    "    for each in whole_dict[target_groups]['annotation'][concept]:\n",
    "        filtered_each = re.findall('(.*)->(.*)', each)\n",
    "        text += ' ' + filtered_each[0][1]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/py36/lib/python3.6/site-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "target_group = 'Abdominal_Disorders'\n",
    "target_group = 'Hip_Replacement'\n",
    "target_group = 'Knee_Problems'\n",
    "\n",
    "target_groups = whole_dict.keys() \n",
    "for target_group in target_groups:\n",
    "    target_concepts = ['UNC','COU', 'CON','NEG']\n",
    "    fig, _axs = plt.subplots(nrows=2, ncols=2, figsize=(50,25))\n",
    "    axs = _axs.flatten()\n",
    "    for index, each in enumerate(target_concepts):\n",
    "        text = collect(whole_dict, target_group, each)\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stopwords, background_color=\"black\").generate(text)\n",
    "        axs[index].set_title(target_group + ' : ' + each, fontsize=50)\n",
    "        axs[index].imshow(wordcloud, interpolation='bilinear')\n",
    "        axs[index].axis(\"off\")\n",
    "    fig.savefig('WordCould_of_Groups_in_Linguistics/'+ target_group + '.png')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New corpus for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isDelays(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    if ' wait ' in sentence \\\n",
    "    or ' delay ' in sentence \\\n",
    "    or ' slow ' in sentence \\\n",
    "    or ' for so long ' in sentence \\\n",
    "    or ' be so long ' in sentence \\\n",
    "    or ' take so long ' in sentence \\\n",
    "    or ' long time ' in sentence \\\n",
    "    or ' longer ' in sentence \\\n",
    "    or 'postpone' in sentence \\\n",
    "    or ' reschedule ' in sentence \\\n",
    "    or ' numerous time' in sentence \\\n",
    "    or ' quicker ' in sentence \\\n",
    "    or '  take forever' in sentence \\\n",
    "    or ' prolong ' in sentence:\n",
    "        return True\n",
    "    if (' ridiculous ' in sentence \n",
    "        or ' unacceptable 'in sentence) and \\\n",
    "    ('months' in sentence \\\n",
    "     or 'days' in sentence \\\n",
    "     or 'weeks' in sentence \\\n",
    "     or 'hours' in sentence \\\n",
    "     or 'years' in sentence \\\n",
    "     or 'month' in sentence \\\n",
    "     or 'day' in sentence \\\n",
    "     or 'week' in sentence \\\n",
    "     or 'hour' in sentence \\\n",
    "     or 'year' in sentence):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isCosts(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    if ' cost ' in sentence \\\n",
    "    or ' costly ' in sentence \\\n",
    "    or ' pay for ' in sentence \\\n",
    "    or 'payment' in sentence \\\n",
    "    or 'afford' in sentence \\\n",
    "    or 'expensive' in sentence \\\n",
    "    or 'cheap' in sentence \\\n",
    "    or ' money ' in sentence \\\n",
    "    or 'limit' in sentence \\\n",
    "    or ' overprice ' in sentence \\\n",
    "    or ' price' in sentence \\\n",
    "    or ' copay' in sentence \\\n",
    "    or ' bill ' in sentence \\\n",
    "    or ' co-pay ' in sentence \\\n",
    "    or ' fee ' in sentence \\\n",
    "    or ' charge ' in sentence \\\n",
    "    or ' deductible ' in sentence \\\n",
    "    or ' premium ' in sentence \\\n",
    "    or 'out-of-network' in sentence \\\n",
    "    or 'out of network' in sentence \\\n",
    "    or 'in-network' in sentence \\\n",
    "    or 'in network' in sentence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isAccess(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    if ' access to ' in sentence \\\n",
    "    or ' be refuse ' in sentence \\\n",
    "    or ' accept new patient ' in sentence \\\n",
    "    or ' be cancel ' in sentence \\\n",
    "    or 'consultation' in sentence \\\n",
    "    or 'registration' in sentence \\\n",
    "    or 'shortage' in sentence \\\n",
    "    or ' crowd ' in sentence \\\n",
    "    or ' reschedule ' in sentence \\\n",
    "    or 'cancellation' in sentence \\\n",
    "    or ' ward ' in sentence \\\n",
    "    or 'available' in sentence \\\n",
    "    or 'reject' in sentence \\\n",
    "    or 'to service' in sentence \\\n",
    "    or 'admission' in sentence \\\n",
    "    or 'out-of-network' in sentence \\\n",
    "    or 'out of network' in sentence \\\n",
    "    or 'in-network' in sentence \\\n",
    "    or 'in network' in sentence \\\n",
    "    or 'limitation' in sentence \\\n",
    "    or ' nearby hospital' in sentence \\\n",
    "    or ' nearby clinic ' in sentence:\n",
    "        return True\n",
    "    if ' limit ' in sentence \\\n",
    "    and 'access' in sentence:\n",
    "        return True\n",
    "    if ' treat' in sentence \\\n",
    "    and ' refus' in sentence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "     \n",
    "def isErrors(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    if ' inept ' in sentence and 'doctor' in sentence:\n",
    "        return True\n",
    "    if ' miss diagnosis ' in sentence \\\n",
    "    or ' mis-diagnosis ' in sentence \\\n",
    "    or ' misdiagnose ' in sentence \\\n",
    "    or ' misdiagnosis ' in sentence \\\n",
    "    or ' clerical ' in sentence \\\n",
    "    or ' erroneous ' in sentence \\\n",
    "    or ' wrong disease ' in sentence \\\n",
    "    or ' typo ' in sentence \\\n",
    "    or ' wrong medi' in sentence \\\n",
    "    or ' medical accident' in sentence \\\n",
    "    or ' incorrect ' in sentence \\\n",
    "    or ' inconclusive ' in sentence \\\n",
    "    or ' fault ' in sentence \\\n",
    "    or ' mislead ' in sentence \\\n",
    "    or ' examination ' in sentence \\\n",
    "    or ' incompetent ' in sentence \\\n",
    "    or 'judgement' in sentence \\\n",
    "    or 'judgment' in sentence \\\n",
    "    or ' miss ' in sentence:\n",
    "        return True\n",
    "    if ' not work ' in sentence:\n",
    "        return True\n",
    "    if ('make' and 'mistake' in sentence) \\\n",
    "    or ' be a mistake' in sentence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def isTreatments(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    if ' get better ' in sentence \\\n",
    "    and ' you ' not in sentence \\\n",
    "    and 'hope' not in sentence \\\n",
    "    and 'life ' not in sentence \\\n",
    "    and ' will ' not in sentence \\\n",
    "    and '?' not in sentence \\\n",
    "    and ' if ' not in sentence \\\n",
    "    and ' to get better ' not in sentence \\\n",
    "    and ' can get better ' not in sentence \\\n",
    "    and ' it ll ' not in sentence \\\n",
    "    and 'i' in sentence:\n",
    "        return True\n",
    "    if ' feel better ' in sentence \\\n",
    "    and ' you ' not in sentence \\\n",
    "    and 'hope' not in sentence \\\n",
    "    and ' will ' not in sentence \\\n",
    "    and '?' not in sentence \\\n",
    "    and ' if ' not in sentence \\\n",
    "    and ' can feel better ' not in sentence \\\n",
    "    and ' would ' not in sentence \\\n",
    "    and ' should feel better' not in sentence \\\n",
    "    and 'i' in sentence:\n",
    "        return True\n",
    "    if ' get worse ' in sentence \\\n",
    "    and ' you ' not in sentence \\\n",
    "    and 'hope' not in sentence \\\n",
    "    and 'life ' not in sentence \\\n",
    "    and ' will ' not in sentence \\\n",
    "    and '?' not in sentence \\\n",
    "    and ' if ' not in sentence \\\n",
    "    and ' to get worse ' not in sentence \\\n",
    "    and ' can get worse ' not in sentence \\\n",
    "    and ' it ll ' not in sentence \\\n",
    "    and 'i' in sentence:\n",
    "        return True\n",
    "    if ' feel worse ' in sentence \\\n",
    "    and ' you ' not in sentence \\\n",
    "    and 'hope' not in sentence \\\n",
    "    and ' will ' not in sentence \\\n",
    "    and '?' not in sentence \\\n",
    "    and ' if ' not in sentence \\\n",
    "    and ' can feel worse ' not in sentence \\\n",
    "    and ' would ' not in sentence \\\n",
    "    and ' should feel worse' not in sentence \\\n",
    "    and 'i' in sentence:\n",
    "        return True\n",
    "\n",
    "    if ' treatment ' in sentence \\\n",
    "    and 'hope' not in sentence:\n",
    "        return True\n",
    "\n",
    "    if ' be never end ' in sentence \\\n",
    "    or ' relapse ' in sentence \\\n",
    "    or ' still sick' in sentence \\\n",
    "    or ' still ill' in sentence \\\n",
    "    or ' recurrent ' in sentence \\\n",
    "    or ' constant pain ' in sentence \\\n",
    "    or ' be cure ' in sentence \\\n",
    "    or ' assessment ' in sentence \\\n",
    "    or ' rehabilitation ' in sentence \\\n",
    "    or ' good therapy ' in sentence \\\n",
    "    or ' great therapy ' in sentence \\\n",
    "    or ' excellent therapy ' in sentence \\\n",
    "    or ' prognosis ' in sentence \\\n",
    "    or (' therapy '  in sentence and ' not work '  in sentence):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "def isStaffAndTrust(sentence):\n",
    "    if len(sentence) < 100 and ('?' in sentence or ' if ' in sentence):\n",
    "        return False\n",
    "    # staff attitude and patient trust\n",
    "    if 'manager' in sentence \\\n",
    "    or 'staff' in sentence \\\n",
    "    or ' nurse' in sentence \\\n",
    "    or 'therapist' in sentence \\\n",
    "    or 'attitude' in sentence \\\n",
    "    or 'unethical' in sentence \\\n",
    "    or 'ethical' in sentence \\\n",
    "    or ' be care' in sentence \\\n",
    "    or 'uncaring' in sentence \\\n",
    "    or 'obnoxious' in sentence \\\n",
    "    or 'lovely' in sentence \\\n",
    "    or 'dismissive' in sentence \\\n",
    "    or ' so kind' in sentence \\\n",
    "    or 'cruel' in sentence \\\n",
    "    or 'disrespectful' in sentence \\\n",
    "    or ' respectful ' in sentence \\\n",
    "    or ' empathetic ' in sentence \\\n",
    "    or ' empathy ' in sentence \\\n",
    "    or ' sympathy ' in sentence \\\n",
    "    or 'sympathetic' in sentence \\\n",
    "    or ' trust' in sentence \\\n",
    "    or 'lack of concern' in sentence \\\n",
    "    or ' rude' in sentence \\\n",
    "    or ' dismiss me' in sentence \\\n",
    "    or 'lack of respect' in sentence \\\n",
    "    or 'polite' in sentence \\\n",
    "    or ' uncaring ' in sentence \\\n",
    "    or ' manner' in sentence \\\n",
    "    or 'so mean' in sentence \\\n",
    "    or 'aggressive' in sentence \\\n",
    "    or 'insensitive' in sentence \\\n",
    "    or 'neglect' in sentence \\\n",
    "    or 'lack of understand' in sentence \\\n",
    "    or 'confidence' in sentence \\\n",
    "    or 'privacy' in sentence:\n",
    "        return True\n",
    "    if ' be understand ' in sentence \\\n",
    "    and 'husband' not in sentence \\\n",
    "    and 'wife' not in sentence \\\n",
    "    and 'employer' not in sentence \\\n",
    "    and 'boyfriend' not in sentence \\\n",
    "    and 'girlfriend' not in sentence \\\n",
    "    and 'i be understand' not in sentence \\\n",
    "    and 'family' not in sentence:\n",
    "        return True\n",
    "\n",
    "    # staff’s professional skills and conduct\n",
    "    if ' skill' in sentence \\\n",
    "    or 'treat well' in sentence \\\n",
    "    or 'treat bad' in sentence \\\n",
    "    or 'unclear' in sentence \\\n",
    "    or ' good treat ' in sentence \\\n",
    "    or ' great treat ' in sentence \\\n",
    "    or ' poor treat ' in sentence \\\n",
    "    or ' bad treat ' in sentence \\\n",
    "    or ' worst treat ' in sentence \\\n",
    "    or ' good care' in sentence \\\n",
    "    or ' great care' in sentence \\\n",
    "     or ' excellent care' in sentence \\\n",
    "    or ' poor care' in sentence \\\n",
    "    or ' bad care' in sentence \\\n",
    "    or ' inept ' in sentence \\\n",
    "    or 'poor communication' in sentence \\\n",
    "    or ' expertise' in sentence \\\n",
    "    or 'fob off' in sentence \\\n",
    "    or 'helpful' in sentence \\\n",
    "    or 'unprofessional' in sentence \\\n",
    "    or 'shout at' in sentence \\\n",
    "    or 'irresponsible' in sentence \\\n",
    "    or 'unqualified' in sentence \\\n",
    "    or 'qualify' in sentence \\\n",
    "    or 'lack of train' in sentence \\\n",
    "    or 'no train' in sentence \\\n",
    "    or 'incapable' in sentence \\\n",
    "    or 'unwilling' in sentence \\\n",
    "    or 'corrupt' in sentence \\\n",
    "    or 'ignore' in sentence \\\n",
    "    or 'mislead' in sentence \\\n",
    "    or 'lack of knowledge' in sentence \\\n",
    "    or ' bad behavi' in sentence \\\n",
    "    or 'not listen' in sentence \\\n",
    "    or 'inappropriate' in sentence \\\n",
    "    or 'dedicate' in sentence \\\n",
    "    or 'very professional' in sentence \\\n",
    "    or 'so professional' in sentence:\n",
    "        return True\n",
    "    if ' look after me' in sentence \\\n",
    "    and 'friend' not in sentence \\\n",
    "    and 'husband' not in sentence \\\n",
    "    and 'wife' not in sentence:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def hasCourse(sentence, cou_realtions):\n",
    "    if len(cou_realtions) > 0 \\\n",
    "    and 'hope' not in sentence \\\n",
    "    and '?' not in sentence \\\n",
    "    and ' will ' not in sentence \\\n",
    "    and ' if ' not in sentence \\\n",
    "    and ' you ' not in sentence:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseSentense(lines, root, text):\n",
    "    sen_index = []\n",
    "    treatment_index = []\n",
    "    test_index = []\n",
    "    cou_of_index = []\n",
    "    sentence_split = []\n",
    "    treatments = []\n",
    "    tests = []\n",
    "    cou_of = []\n",
    "    for line in lines:\n",
    "        if line[:11] == 'NamedEntity' and line[31:40] == 'treatment':\n",
    "            treatment_index.append([int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2]), line.split(\"\\t\")[7][3:].strip()])\n",
    "        if line[:11] == 'NamedEntity' and line[31:35] == 'test':\n",
    "            test_index.append([int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2]), line.strip().split(\"\\t\")[7][3:].strip()])\n",
    "        if line[:8] == 'Sentence':\n",
    "            sen_index.append([int(line.split(\"\\t\")[1]), int(line.split(\"\\t\")[2])])\n",
    "    \n",
    "    entity_dict = {}\n",
    "    for child in root:\n",
    "        if 'ClampNameEntityUIMA' in child.tag and child.attrib['semanticTag'] in set([\"COU\", \"problem\"]):\n",
    "            entity_dict[child.attrib['{http://www.omg.org/XMI}id']] = {\"begin-end\": (int(child.attrib['begin']), int(child.attrib['end'])), \"Semantic\": child.attrib['semanticTag']}\n",
    "            \n",
    "        if 'ClampRelationUIMA' in child.tag and child.attrib['semanticTag'] == \"COU_Of\":\n",
    "            cou_of_index.append((entity_dict[child.attrib['entTo']][\"begin-end\"][0],\n",
    "                                entity_dict[child.attrib['entTo']][\"begin-end\"][1], \n",
    "                                entity_dict[child.attrib['entFrom']][\"begin-end\"][0],\n",
    "                                entity_dict[child.attrib['entFrom']][\"begin-end\"][1]))\n",
    "    for index in sen_index:\n",
    "        sentence_split.append(text[index[0]:index[1]])\n",
    "        treatment = []\n",
    "        for t_index in treatment_index:\n",
    "            if t_index[0] >= index[0] and t_index[1] <= index[1]:\n",
    "                treatment.append(t_index[2])\n",
    "        treatments.append(treatment)\n",
    "                                \n",
    "        test = []\n",
    "        for te_index in test_index:\n",
    "            if te_index[0] >= index[0] and te_index[1] <= index[1]:\n",
    "                test.append(te_index[2])\n",
    "        tests.append(test)\n",
    "        \n",
    "        cou = []\n",
    "        for cou_index in cou_of_index:\n",
    "            if cou_index[0] >= index[0] \\\n",
    "            and cou_index[1] <= index[1] \\\n",
    "            and cou_index[2] >= index[0] \\\n",
    "            and cou_index[3] <= index[1]:\n",
    "                cou.append(text[cou_index[0]:cou_index[1]] + '->' + text[cou_index[2]:cou_index[3]])\n",
    "        cou_of.append(cou)\n",
    "    return (sentence_split, treatments, tests, cou_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "B\n",
      "C\n",
      "D\n",
      "E\n",
      "F\n",
      "G\n",
      "H\n",
      "I\n",
      "K\n",
      "L\n",
      "M\n",
      "N\n",
      "O\n",
      "P\n",
      "Q\n",
      "R\n",
      "S\n",
      "T\n",
      "U\n",
      "V\n",
      "W\n"
     ]
    }
   ],
   "source": [
    "raw_text_path = \"/Volumes/E/ClampCmd_1.5.1/input\"\n",
    "entity_annotation_path = \"/Volumes/E/ClampCmd_1.5.1/output\"\n",
    "sentense_dic  = []\n",
    "countforannotation = {'isDelays': 0, 'isCosts': 0, 'isAccess': 0, 'isErrors': 0, 'isTreatments': 0, 'isStaffAndTrust': 0}\n",
    "\n",
    "for files in os.listdir(raw_text_path):\n",
    "    if '.' not in files:\n",
    "        print(files)\n",
    "        for filename in os.listdir(raw_text_path + '/' + files):\n",
    "            if filename[-4:] == \".txt\":\n",
    "                text_f = open(raw_text_path + '/' + files + '/' + filename, 'r')\n",
    "                endIndex = re.search('-',filename).span()[0]\n",
    "                group = filename[:endIndex]\n",
    "                text = text_f.readline()\n",
    "                text_f.close()\n",
    "            \n",
    "                anno_f = open(entity_annotation_path + '/' + filename[0] + \"/\" + filename, 'r')\n",
    "                tree = ET.parse(entity_annotation_path + '/' + filename[0] + \"/\" + filename[:-4] + '.xmi')\n",
    "                root = tree.getroot()\n",
    "                sentences, treatments, test, cou_realtions = parseSentense(anno_f.readlines(), root, text)\n",
    "                anno_f.close()\n",
    "                count = 0\n",
    "                for sen, treat, te, cou_realtion in zip(sentences, treatments, test, cou_realtions):\n",
    "                    count += 1\n",
    "                    trainOrtest = 'test'\n",
    "                    train_class = []\n",
    "                    if isDelays(sen):\n",
    "                        countforannotation['isDelays'] += 1\n",
    "                        train_class.append(1)\n",
    "                        trainOrtest = 'train'\n",
    "                    if isCosts(sen):\n",
    "                        countforannotation['isCosts'] += 1\n",
    "                        train_class.append(2)\n",
    "                        trainOrtest = 'train'\n",
    "                    if isAccess(sen):\n",
    "                        countforannotation['isAccess'] += 1\n",
    "                        train_class.append(3)\n",
    "                        trainOrtest = 'train'\n",
    "                    if isErrors(sen):\n",
    "                        countforannotation['isErrors'] += 1\n",
    "                        train_class.append(4)\n",
    "                        trainOrtest = 'train'\n",
    "                    if isTreatments(sen) or hasCourse(sen, cou_realtion):\n",
    "                        countforannotation['isTreatments'] += 1\n",
    "                        train_class.append(5)\n",
    "                        trainOrtest = 'train'\n",
    "                    if isStaffAndTrust(sen):\n",
    "                        countforannotation['isStaffAndTrust'] += 1\n",
    "                        train_class.append(6)\n",
    "                        trainOrtest = 'train'\n",
    "                    sentense_dic.append({'group': group, 'id': filename[:-4] + '-' +str(count), 'text': sen, 'treatment': treat, 'couse_of_problem': cou_realtion, 'test': te, 'trainOrtest': trainOrtest, 'aspect': train_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_df = pd.DataFrame(sentense_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aspect</th>\n",
       "      <th>couse_of_problem</th>\n",
       "      <th>group</th>\n",
       "      <th>id</th>\n",
       "      <th>test</th>\n",
       "      <th>text</th>\n",
       "      <th>trainOrtest</th>\n",
       "      <th>treatment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>Abdominal_Disorders--12291-0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>i be 34 have have a stoma since 28.</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>Abdominal_Disorders--12291-0-2</td>\n",
       "      <td>[]</td>\n",
       "      <td>it have ruin my life and my life be over .</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>Abdominal_Disorders--12291-0-3</td>\n",
       "      <td>[]</td>\n",
       "      <td>there be no help for us we be on our own compl...</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>Abdominal_Disorders--12291-1-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>it be the embarrasment or other people embarra...</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>Abdominal_Disorders</td>\n",
       "      <td>Abdominal_Disorders--12291-1-2</td>\n",
       "      <td>[]</td>\n",
       "      <td>etc .</td>\n",
       "      <td>test</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  aspect couse_of_problem                group  \\\n",
       "0     []               []  Abdominal_Disorders   \n",
       "1     []               []  Abdominal_Disorders   \n",
       "2     []               []  Abdominal_Disorders   \n",
       "3     []               []  Abdominal_Disorders   \n",
       "4     []               []  Abdominal_Disorders   \n",
       "\n",
       "                               id test  \\\n",
       "0  Abdominal_Disorders--12291-0-1   []   \n",
       "1  Abdominal_Disorders--12291-0-2   []   \n",
       "2  Abdominal_Disorders--12291-0-3   []   \n",
       "3  Abdominal_Disorders--12291-1-1   []   \n",
       "4  Abdominal_Disorders--12291-1-2   []   \n",
       "\n",
       "                                                text trainOrtest treatment  \n",
       "0                i be 34 have have a stoma since 28.        test        []  \n",
       "1         it have ruin my life and my life be over .        test        []  \n",
       "2  there be no help for us we be on our own compl...        test        []  \n",
       "3  it be the embarrasment or other people embarra...        test        []  \n",
       "4                                              etc .        test        []  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "whole_df.to_csv('whole_sentence_level.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences to be annotaed: 582358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'isDelays': 153881,\n",
       " 'isCosts': 63945,\n",
       " 'isAccess': 29360,\n",
       " 'isErrors': 52332,\n",
       " 'isTreatments': 135823,\n",
       " 'isStaffAndTrust': 147017}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Total sentences to be annotaed:', sum(countforannotation.values()))\n",
    "countforannotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "isDelays_ = []\n",
    "isCosts_ = []\n",
    "isAccess_ = []\n",
    "isErrors_ = []\n",
    "isTreatments_ = []\n",
    "isStaffAndTrust_ = []\n",
    "for each in sentense_dic:\n",
    "    if 1 in each['aspect']:\n",
    "        isDelays_.append(each)\n",
    "    if 2 in each['aspect']:\n",
    "        isCosts_.append(each)\n",
    "    if 3 in each['aspect']:\n",
    "        isAccess_.append(each)\n",
    "    if 4 in each['aspect']:\n",
    "        isErrors_.append(each)\n",
    "    if 5 in each['aspect']:\n",
    "        isTreatments_.append(each)\n",
    "    if 6 in each['aspect']:\n",
    "        isStaffAndTrust_.append(each)\n",
    "        \n",
    "isDelays_df = pd.DataFrame(isDelays_)\n",
    "isCosts_df = pd.DataFrame(isCosts_)\n",
    "isAccess_df = pd.DataFrame(isAccess_)  \n",
    "isErrors_df = pd.DataFrame(isErrors_)  \n",
    "isTreatments_df = pd.DataFrame(isTreatments_)  \n",
    "isStaffAndTrust_df = pd.DataFrame(isStaffAndTrust_) \n",
    "\n",
    "isDelays_df['sub_aspect'] = ''\n",
    "isDelays_df['sentiment'] = ''\n",
    "isCosts_df['sub_aspect'] = ''\n",
    "isCosts_df['sentiment'] = ''\n",
    "isAccess_df['sub_aspect'] = ''\n",
    "isAccess_df['sentiment'] = ''\n",
    "isErrors_df['sub_aspect'] = ''\n",
    "isErrors_df['sentiment'] = ''\n",
    "isTreatments_df['sub_aspect'] = ''\n",
    "isTreatments_df['sentiment'] = ''\n",
    "isStaffAndTrust_df['sub_aspect'] = ''\n",
    "isStaffAndTrust_df['sentiment'] = ''\n",
    "\n",
    "isDelays_df_rest, isDelays_df_sample = train_test_split(isDelays_df, test_size=0.1, random_state=1)\n",
    "isCosts_df_rest1, isCosts_df_sample = train_test_split(isCosts_df, test_size=0.1, random_state=1)\n",
    "isAccess_df_rest, isAccess_df_sample = train_test_split(isAccess_df, test_size=0.1, random_state=1)\n",
    "isErrors_df_rest, isErrors_df_sample = train_test_split(isErrors_df, test_size=0.1, random_state=1)\n",
    "isTreatments_df_rest, isTreatments_df_sample = train_test_split(isTreatments_df, test_size=0.1, random_state=1)\n",
    "isStaffAndTrust_df_rest, isStaffAndTrust_df_sample = train_test_split(isStaffAndTrust_df, test_size=0.1, random_state=1)\n",
    "\n",
    "isDelays_df_sample.to_csv('isDelays_sample.csv', index=False)\n",
    "isCosts_df_sample.to_csv('isCosts_sample.csv', index=False)\n",
    "isAccess_df_sample.to_csv('isAccess_sample.csv', index=False)\n",
    "isErrors_df_sample.to_csv('isErrors_sample.csv', index=False)\n",
    "isTreatments_df_sample.to_csv('isTreatments_sample.csv', index=False)\n",
    "isStaffAndTrust_df_sample.to_csv('isStaffAndTrust_sample.csv', index=False)\n",
    "\n",
    "# isDelays_df_rest.to_csv('isDelays_rest.csv', index=False)\n",
    "# isCosts_df_rest.to_csv('isCosts_rest.csv', index=False)\n",
    "# isAccess_df_rest.to_csv('isAccess_rest.csv', index=False)\n",
    "# isErrors_df_rest.to_csv('isErrors_rest.csv', index=False)\n",
    "# isTreatments_df_rest.to_csv('isTreatments_rest.csv', index=False)\n",
    "# isStaffAndTrust_df_rest.to_csv('isStaffAndTrust_rest.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = len(isDelays_df_sample)\n",
    "batch = (int) (n / 10)\n",
    "isDelays_df_sample_tenFold = pd.DataFrame()\n",
    "for i in range(1, 11):\n",
    "    batch_df = isDelays_df_sample[batch * (i - 1):batch * i]\n",
    "    isDelays_df_sample_tenFold['group-' + str(i)] = list(batch_df['group'])\n",
    "    isDelays_df_sample_tenFold['id-' + str(i)] = list(batch_df['id'])\n",
    "    isDelays_df_sample_tenFold['text-' + str(i)] = list(batch_df['text'])\n",
    "    isDelays_df_sample_tenFold['trainOrtest-' + str(i)] = list(batch_df['trainOrtest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142617"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(isDelays_df_rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_df = pd.read_json(json.dumps(sentense_dic), orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8389152"
      ]
     },
     "execution_count": 712,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentense_dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
